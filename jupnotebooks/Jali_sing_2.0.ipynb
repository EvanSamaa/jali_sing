{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/evan1/Documents/JALI_sing_stuff/jali_sing')\n",
    "from util.SongDataStructure import *\n",
    "from util.pitch_interval_estimation import *\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from models.vowel_modification_detector import vowel_mod_detector\n",
    "vowel_mod = vowel_mod_detector(model_dir=\"C:/Users/evan1/Documents/JALI_sing_stuff/jali_sing/models/over_trained_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep2_adale\"\n",
    "file_name_template = \"audio_vocals\"\n",
    "script_path = os.path.join(dir, \"audio_vocals_fixed.TextGrid\")\n",
    "output_template = \"jali_sing_cardinal\"\n",
    "spike_width = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicionaries and Constants and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOICED = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW', \"ER\"])\n",
    "CMU2VISEME = {\"AA\":\"Ah\", \"AO\":\"Ah\", \"AY\":\"Ah\", \"AW\":\"Ah\",\"AE\":\"Aa\",\n",
    "              \"EY\":\"Aa\",\"UH\":\"Uh\", \"UW\":\"U\",\"IH\": \"Ih\",\"IY\": \"Ih\",\"EH\": \"Eh\",\"HH\": \"Eh\",\"UH\": \"Eh\",\"AH\": \"Eh\",\n",
    "              \"ER\": \"Eh\",\"OW\":\"Oo\",\"OY\":\"Oh\",\"R\":\"R\",\"D\":\"LNTD\",\"T\": \"LNTD\",\"L\":\"LNTD\",\"N\":\"LNTD\",\"NG\":\"LNTD\",\n",
    "              \"F\":\"FV\",\"V\":\"FV\",\"B\":\"BP\",\"M\":\"M\",\"P\":\"BP\",\"CH\":\"ShChZh\",\"SH\":\"ShChZh\",\"ZH\":\"ShChZh\",\n",
    "              \"S\": \"SZ\", \"Z\": \"SZ\",\"DH\":\"Th\", \"TH\":\"Th\",\"G\":\"GK\", \"K\":\"GK\",\"Y\":\"Y\",\"JH\":\"J\",\"W\":\"W\",}\n",
    "VOWELS_SLIDERS_JALI = set(['Ih_pointer', 'Ee_pointer', 'Eh_pointer', 'Aa_pointer', 'U_pointer', 'Uh_pointer'\n",
    "                           , 'Oo_pointer', 'Oh_pointer', 'Schwa_pointer', 'Eu_pointer', \"Ah_pointer\"])\n",
    "CONSONANTS_SLIDERS_JALI = set([\"M_pointer\", \"BP_pointer\", \"JY_pointer\", \"Th_pointer\", \"ShChZh_pointer\", \"SZ_pointer\", \"GK_pointer\", \"LNTD_pointer\", \"R_pointer\", \"W_pointer\", \"FV_pointer\"])\n",
    "CONSONANTS_SLIDERS_NOJAW_JALI = set([\"Ya_pointer\", \"Ja_pointer\", \"Ra_pointer\", \"FVa_pointer\", \"LNTDa_pointer\", \"Ma_pointer\", \"BPa_pointer\", \"Wa_pointer\", \"Tha_pointer\", \"GKa_pointer\"])\n",
    "JALI_SLIDERS_SET = set.union(VOWELS_SLIDERS_JALI, CONSONANTS_SLIDERS_JALI, CONSONANTS_SLIDERS_NOJAW_JALI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying only with the cardinal vowels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU2VISEME = {\"AA\":\"Ah\", \"AO\":\"Ah\", \"AY\":\"Ah\", \"AW\":\"Ah\",\"AE\":\"Eh\",\n",
    "              \"EY\":\"Eh\",\"UH\":\"Ah\", \"UW\":\"U\",\"IH\": \"Ee\",\"IY\": \"Ee\",\"EH\": \"Eh\",\"HH\": \"Eh\",\"UH\": \"U\",\"AH\": \"Eh\",\n",
    "              \"ER\": \"Eh\",\"OW\":\"Oo\",\"OY\":\"Oh\",\"R\":\"R\",\"D\":\"LNTD\",\"T\": \"LNTD\",\"L\":\"LNTD\",\"N\":\"LNTD\",\"NG\":\"LNTD\",\n",
    "              \"F\":\"FV\",\"V\":\"FV\",\"B\":\"BP\",\"M\":\"M\",\"P\":\"BP\",\"CH\":\"ShChZh\",\"SH\":\"ShChZh\",\"ZH\":\"ShChZh\",\n",
    "              \"S\": \"SZ\", \"Z\": \"SZ\",\"DH\":\"Th\", \"TH\":\"Th\",\"G\":\"GK\", \"K\":\"GK\",\"Y\":\"Y\",\"JH\":\"J\",\"W\":\"W\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMU_phonemes_dicts():\n",
    "    def __init__(self):\n",
    "        self.vocabs = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G',\n",
    "                  'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "                  'UW', 'V', 'W', 'Y', 'Z', 'ZH'])\n",
    "        self.vowels = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', \n",
    "                  'IH', 'IY', 'OW', 'OY', 'UH', 'UW', ])\n",
    "        self.voiced = set(['M', 'N']).union(self.vowels)\n",
    "        self.consonants = set(['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', \n",
    "                              'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH'])\n",
    "        self.consonants_no_jaw = self.consonants\n",
    "        self.lip_closer = set([\"B\", \"F\", \"M\", \"P\", \"S\", \"V\"])\n",
    "        self.lip_rounder = set([\"B\", \"F\", \"M\", \"P\", \"V\"])\n",
    "        self.nasal_obtruents = set(['L', 'N', 'NG', 'T', 'D', 'G', 'K', 'F', 'V', 'M', 'B', 'P'])\n",
    "        self.fricative = set([\"S\", \"Z\", \"ZH\", \"SH\", \"CH\", \"F\", \"V\", 'TH'])\n",
    "        self.plosive = set([\"P\", \"B\", \"D\", \"T\", \"K\", \"G\"])\n",
    "        self.lip_heavy = set([\"W\", \"OW\", \"UW\", \"S\", \"Z\", \"Y\", \"JH\", \"OY\"])\n",
    "        self.sibilant = set([\"S\", \"Z\", \"SH\", \"CH\", \"ZH\"])\n",
    "class JALI_visemes_dicts():\n",
    "     def __init__(self):\n",
    "        self.vowels = set(['Ih_pointer', 'Ee_pointer', 'Eh_pointer', 'Aa_pointer', 'U_pointer', 'Uh_pointer'\n",
    "                           , 'Oo_pointer', 'Oh_pointer', 'Schwa_pointer', 'Eu_pointer', \"Ah_pointer\"])\n",
    "        self.voiced = set(['Ih_pointer', 'Ee_pointer', 'Eh_pointer', 'Aa_pointer', 'U_pointer', 'Uh_pointer'\n",
    "                           , 'Oo_pointer', 'Oh_pointer', 'Schwa_pointer', 'Eu_pointer', \"Ah_pointer\", \"LNTD_pointer\", \"LNTDa_pointer\"])\n",
    "        self.consonants_no_jaw = set([\"Ya_pointer\", \"Ja_pointer\", \"Ra_pointer\", \"FVa_pointer\", \"LNTDa_pointer\", \"Ma_pointer\", \"BPa_pointer\", \"Wa_pointer\", \"Tha_pointer\", \"GKa_pointer\"])\n",
    "        self.consonants = set([\"M_pointer\", \"BP_pointer\", \"JY_pointer\", \"Th_pointer\", \"ShChZh_pointer\", \"SZ_pointer\", \"GK_pointer\", \"LNTD_pointer\", \"R_pointer\", \"W_pointer\", \"FV_pointer\"]) \n",
    "        self.lip_closer = set([\"M_pointer\", \"BP_pointer\", \"FV_pointer\", \"SZ_pointer\"])\n",
    "        self.lip_rounder = set([\"M_pointer\", \"BP_pointer\", \"FV_pointer\"])\n",
    "        self.vocabs = self.consonants.union(self.vowels).union(self.consonants_no_jaw)\n",
    "        self.sibilant = set([\"SZ_pointer\", \"ShChZh_pointer\"])\n",
    "        self.nasal_obtruents = set([\"LNTD_pointer\", \"GK_pointer\", \"FV_pointer\", \"M_pointer\", \"BP_pointer\"])\n",
    "        self.fricative = set([\"FV_pointer\", \"SZ_pointer\", \"ShChZh_pointer\", \"Th_pointer\"])\n",
    "        self.plosive = set([\"BP_pointer\", \"LNTDa_pointer\", \"GK_pointer\"])\n",
    "        self.lip_heavy = set([\"Oh_pointer\", \"W_pointer\", \"Wa_pointer\", \"U_pointer\", \"SZ_pointer\", \"JY_pointer\",\n",
    "                             \"Ya_pointer\", \"Ja_pointer\"])\n",
    "        self.lip_rounder_to_no_jaw_dict = {\"M_pointer\":\"Ma_pointer\", \"BP_pointer\":\"BPa_pointer\", \"FV_pointer\":\"FVa_pointer\"}\n",
    "cmu_sets = CMU_phonemes_dicts()\n",
    "jali_sets = JALI_visemes_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_viseme_curve(start, end, value, sustain=1, decay = 0.75, onset=0.1, offset=0):\n",
    "    if end - start < 0.1:\n",
    "        end = start + 0.1\n",
    "    interval = []\n",
    "    interval.append([start-onset, 0])\n",
    "    # second point is when the belting starts \n",
    "    interval.append([start, 1 * value])\n",
    "    # third point emphasizes decay, it happens 75% down the interval\n",
    "    if sustain < 1:\n",
    "        interval.append([start + sustain * (end - start), decay * value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    elif sustain == 1:\n",
    "        interval.append([end, value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    return interval\n",
    "def get_kth_neighbour(input_list, i, k):\n",
    "    if i+k < 0 or i+k >= len(input_list):\n",
    "        return None\n",
    "    return input_list[i+k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tanh function to get a smooth curve\n",
    "def Viseme_A(peak=None, lowest=None):\n",
    "    if not peak is None:\n",
    "        total = np.log((1+peak)/(1-peak))/2\n",
    "        b = np.log((1+lowest)/(1-lowest))/2\n",
    "        a = total-b\n",
    "    else:\n",
    "        peak = 0.99\n",
    "        lowest = 0.4\n",
    "        total = np.log((1+peak)/(1-peak))/2\n",
    "        b = np.log((1+lowest)/(1-lowest))/2\n",
    "        a = total-b\n",
    "    def fn(val, val_max, val_min, max_val = 10):\n",
    "        val = (val - val_min) / (val_max - val_min)\n",
    "#         print(val)\n",
    "#         return (lowest + val * ((peak - lowest)))*max_val\n",
    "        return np.tanh((val)*a+b) * max_val\n",
    "#         return (np.exp(val * 8)/np.exp(8) * (peak-lowest) + lowest) * max_val\n",
    "    return fn\n",
    "viseme_A = Viseme_A()\n",
    "# plt.plot(np.arange(0, 1, 0.01), viseme_A(np.arange(0, 1, 0.01), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load file for Jali\n",
    "lyric = Minimal_song_data_structure(os.path.join(dir, file_name_template+\".wav\"), os.path.join(dir, file_name_template+\".txt\"),\n",
    "                                                                                             script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# break the thing into sentence structures (if possible)\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes\n",
    "if len(sentences) == 0:\n",
    "    sentences = [list(range(0, len(lyric.phoneme_list)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_list = lyric.phoneme_list\n",
    "phoneme_interval = lyric.phoneme_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass 1, Determine Animation Curve for all the vowels (without consideration for co-articulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2644049337356678\n",
      "3.2194049337356687\n",
      "3.204404933735669\n",
      "3.15440493373567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_18176/136409641.py:49: RuntimeWarning: Mean of empty slice.\n",
      "  yf[vib_int[0]:vib_int[1]] = yf[vib_int[0]:vib_int[1]].mean()\n",
      "C:\\Users\\evansamaa\\anaconda3\\envs\\visemenet\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.692374999999997\n",
      "8.632374999999998\n",
      "8.672374999999997\n",
      "8.572375\n",
      "8.612374999999998\n",
      "8.512375\n",
      "8.572375\n",
      "8.452375000000002\n",
      "8.522375\n",
      "8.392375000000003\n",
      "8.512375\n",
      "8.332375000000004\n",
      "8.502375\n",
      "8.272375000000006\n"
     ]
    }
   ],
   "source": [
    "# animate only vowels and see how it goes\n",
    "viseme_list = []\n",
    "viseme_intervals = []\n",
    "pure_phoneme_list = []\n",
    "max_activation = 8\n",
    "# threshold_slope = 200 # for formants\n",
    "threshold_slope = 80 # for pitch\n",
    "\n",
    "for sentence in sentences:\n",
    "    lyric.pitch.xs()[0]\n",
    "    sentence_pitch_x, sentence_pitch_y = lyric.get_f_interval([max(phoneme_interval[sentence[0]][0], lyric.pitch.xs()[0]), \n",
    "                                                               phoneme_interval[sentence[-1]][1]])\n",
    "    sentence_percentiles = np.nanpercentile(sentence_pitch_y, [5, 95])\n",
    "    sentence_mean = sentence_pitch_y.mean()\n",
    "    sentence_min = sentence_percentiles[0]\n",
    "    sentence_max = sentence_percentiles[1]\n",
    "    for i in range(0, len(sentence)):\n",
    "        # skip the misc symbols in the sentence\n",
    "        if phoneme_list[sentence[i]] != \"EOL_tag\" and phoneme_list[sentence[i]] != \">\":\n",
    "            pure_phoneme_list.append(phoneme_list[sentence[i]])\n",
    "        if phoneme_list[sentence[i]] in VOICED:\n",
    "            xI, yI = lyric.get_I_interval(phoneme_interval[sentence[i]])\n",
    "            xf, yf = lyric.get_f_interval(phoneme_interval[sentence[i]])\n",
    "            xF, yF = lyric.get_F1_interval(phoneme_interval[sentence[i]])\n",
    "            length_of_interval = xI[-1] - xI[0] \n",
    "            if length_of_interval <= 0.30: # if the vowel is approximately a speech vowel, then it is handled like speech\n",
    "                onset = 0.12\n",
    "                offset = 0.12\n",
    "                if phoneme_list[sentence[i]] in cmu_sets.lip_heavy:\n",
    "                    onset = 0.16\n",
    "                    offset = 0.16\n",
    "                value = 7\n",
    "                sustain = 0.75\n",
    "                decay = 0.75\n",
    "                curve = generate_basic_viseme_curve(phoneme_interval[sentence[i]][0], phoneme_interval[sentence[i]][1], value, sustain=sustain, \n",
    "                                            decay = decay, onset=onset, offset=offset)\n",
    "                viseme_intervals.append(curve)\n",
    "                viseme_list.append(CMU2VISEME[phoneme_list[sentence[i]]] + \"_pointer\")\n",
    "            else:\n",
    "                onset = 0.12\n",
    "                offset = 0.12\n",
    "                if phoneme_list[sentence[i]] in cmu_sets.lip_heavy:\n",
    "                    onset = 0.16\n",
    "                    offset = 0.16\n",
    "                control_pts = []\n",
    "                vib = lyric.compute_vibrato_intervals(yf-savgol_filter(yf, 29, 1), xf, lyric.dt)\n",
    "                vib_interval_indexs = lyric.get_subarrays_indexes_from_time_interval(vib, xf)\n",
    "                for vib_int in vib_interval_indexs:\n",
    "                    yf[vib_int[0]:vib_int[1]] = yf[vib_int[0]:vib_int[1]].mean()\n",
    "                slopes_f, intervals_f = efficient_piece_wise_linear_intervals(xf, yf)\n",
    "                kpx_f, kpy_f = get_key_points(xf, yf, intervals_f, slopes_f)\n",
    "                # onset and offset of these would be the same as regular vowels \n",
    "                control_pts.append([phoneme_interval[sentence[i]][0] - onset, 0])\n",
    "                \n",
    "                # here I will determine the average pitch when singing this vowel. Which is used to \n",
    "                avg_pitch = 0\n",
    "                total_weight = 0\n",
    "                pitch_values = []\n",
    "                for si in range(0, len(slopes_f)):\n",
    "                    if abs(slopes_f[si]) <= threshold_slope:\n",
    "                        avg_pitch = avg_pitch + (yf[intervals_f[si][0]] * (intervals_f[si][1] - intervals_f[si][0]))\n",
    "                        pitch_values.append(yf[intervals_f[si][0]])\n",
    "                        total_weight = total_weight + (intervals_f[si][1] - intervals_f[si][0])\n",
    "                if avg_pitch == 0:\n",
    "                        avg_pitch = yf.mean()\n",
    "                        total_weight = 1\n",
    "                pitch_values = np.array(pitch_values)\n",
    "                avg_pitch = avg_pitch / total_weight\n",
    "                \n",
    "                # now find the first key point - i.e. the beginning of the first plateau\n",
    "                # the first key point is defined as the point where a plateau first appears \n",
    "                # it can also be the first point that reaches the same pitch as the first plateau\n",
    "                # this ensures that undetected vibrato will not mess up with the timing\n",
    "                # and cause the mouth to open too slowly \n",
    "                start = 0\n",
    "                for si in range(0, len(slopes_f)):\n",
    "                    if abs(slopes_f[si]) <= threshold_slope:\n",
    "                        interp_pitch = interp1d(kpx_f, kpy_f)\n",
    "                        start = si\n",
    "                        end_x_search = min(xf[intervals_f[si][0]], kpx_f[-1])\n",
    "                        begin_x_search = xf[intervals_f[0][0]]\n",
    "                        x_range = np.arange(begin_x_search, end_x_search, 0.01)\n",
    "                        f_range = interp_pitch(x_range)\n",
    "                        first_val = interp_pitch(xf[intervals_f[si][0]])\n",
    "                        for ssi in range(0, x_range.shape[0]-1):\n",
    "                            if ((f_range[ssi] < first_val) and  (f_range[ssi+1] >= first_val) \n",
    "                                or (f_range[ssi] >= first_val) and  (f_range[ssi+1] < first_val)):\n",
    "                                end_x_search = x_range[ssi]\n",
    "                                break\n",
    "                        val0 = viseme_A(yf[intervals_f[si][0]], yf.max(), min(yf.min(), sentence_mean))\n",
    "                        val1 = viseme_A((yf[intervals_f[si][1]] + yf[intervals_f[si][0]])/2, yf.max(), min(yf.min(), sentence_mean))\n",
    "\n",
    "#                         dif = (xf[intervals_f[si][1]] - end_x_search) * 0.7\n",
    "                        dif = max((xf[intervals_f[si][1]] - xf[intervals_f[si][0]]) * 0.8, xf[intervals_f[si][1]] - xf[intervals_f[si][0]]- 0.1)\n",
    "                        control_pts.append([end_x_search, val0])\n",
    "                        control_pts.append([xf[intervals_f[si][0]] + dif, val1])                 \n",
    "                        break\n",
    "                # now determine intermediate key-points that correlates with lipshape\n",
    "                # will not have any of these pts\n",
    "#                 for si in range(start+1, len(slopes_f)):\n",
    "#                     val0 = viseme_A(yf[intervals_f[si][0]], yf.max(), min(yf.min(), sentence_mean))\n",
    "#                     val1 = viseme_A((yf[intervals_f[si][1]] + yf[intervals_f[si][0]])/2, yf.max(), min(yf.min(), sentence_mean))\n",
    "#                     dif = max((xf[intervals_f[si][1]] - xf[intervals_f[si][0]]) * 0.8, xf[intervals_f[si][1]] - xf[intervals_f[si][0]]- 0.1)\n",
    "#                     if abs(slopes_f[si]) <= threshold_slope:\n",
    "#                         control_pts.append([xf[intervals_f[si][0]], val0])\n",
    "#                         control_pts.append([xf[intervals_f[si][0]] + dif, val1])\n",
    "                    \n",
    "                for si in range(len(slopes_f)-1, -1, -1):\n",
    "                    if abs(slopes_f[si]) <= threshold_slope:\n",
    "                        control_pts.append([xf[intervals_f[si][1]], viseme_A(yf[intervals_f[si][1]], \n",
    "                                                                             yf.max(), yf.min())])\n",
    "                        break\n",
    "                control_pts.append([phoneme_interval[sentence[i]][1] + offset, 0])\n",
    "                viseme_intervals.append(control_pts)\n",
    "                viseme_list.append(CMU2VISEME[phoneme_list[sentence[i]]] + \"_pointer\")   \n",
    "                \n",
    "        ### Dealing with consonants here. I'm just going to insert them between without too much modification\n",
    "        ### it will about the same as pure Jali\n",
    "        elif phoneme_list[sentence[i]] in cmu_sets.consonants:\n",
    "            onset = 0.12\n",
    "            offset = 0.12\n",
    "            if CMU2VISEME[phoneme_list[sentence[i]]] in jali_sets.nasal_obtruents and phoneme_interval[sentence[i]][1] - phoneme_interval[sentence[i]][0] > 1/20:\n",
    "                viseme_jali = CMU2VISEME[phoneme_list[sentence[i]]] + \"_pointer\"\n",
    "            else:\n",
    "                if phoneme_list[sentence[i]] == \"HH\" or phoneme_list[sentence[i]] in cmu_sets.sibilant:\n",
    "                    viseme_jali = CMU2VISEME[phoneme_list[sentence[i]]] + \"_pointer\"\n",
    "                else:\n",
    "                    viseme_jali = CMU2VISEME[phoneme_list[sentence[i]]] + \"a_pointer\"\n",
    "            if viseme_jali in jali_sets.lip_heavy:\n",
    "                onset = 0.16\n",
    "                offset = 0.16\n",
    "            start = phoneme_interval[sentence[i]][0]\n",
    "            end = phoneme_interval[sentence[i]][1]\n",
    "            if (end - start) <= 0.1:\n",
    "                value = 6\n",
    "                sustain = 0.75\n",
    "                decay = 0.75\n",
    "            elif (end - start) <= 0.3:\n",
    "                value = 6\n",
    "                sustain = 0.75\n",
    "                decay = 0.75\n",
    "            else:\n",
    "                value = 8\n",
    "                sustain = 0.75\n",
    "                decay = 0.75\n",
    "            if phoneme_list[sentence[i]] in cmu_sets.lip_closer:\n",
    "                value = 10\n",
    "            viseme_curve = generate_basic_viseme_curve(start, end, value, sustain=sustain, decay=decay, onset=onset, offset=offset)\n",
    "            viseme_list.append(viseme_jali)\n",
    "            viseme_intervals.append(viseme_curve)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pass 2 for co-articupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce co-articulation rules of consonants\n",
    "viseme_list_final = []\n",
    "viseme_intervals_final = []\n",
    "pure_phoneme_list_final = []\n",
    "i = 0;\n",
    "\n",
    "while i < len(viseme_list):\n",
    "    increment = 1\n",
    "    i_next = min(i + 1, len(viseme_list)-1)\n",
    "    if (viseme_list[i_next] == viseme_list[i] or (pure_phoneme_list[i] == \"HH\" and pure_phoneme_list[i_next] in cmu_sets.vowels)\n",
    "        and viseme_intervals[i][-1][0] >= viseme_intervals[i_next][0][0]):\n",
    "        # remove repeated vowels or consonants\n",
    "        viseme_list_final.append(viseme_list[i_next])\n",
    "        int_curr = viseme_intervals[i]\n",
    "        int_next = viseme_intervals[i_next]\n",
    "        viseme_interval = [int_curr[0], [int_curr[1][0], max(int_curr[1][1], int_next[1][1])]]\n",
    "        for interv in int_next[2:]:\n",
    "            viseme_interval.append(interv)\n",
    "        viseme_intervals_final.append(viseme_interval)\n",
    "        pure_phoneme_list_final.append(pure_phoneme_list[i_next])\n",
    "        if viseme_list[i_next] in jali_sets.lip_rounder:\n",
    "            viseme_list_final.append(jali_sets.lip_rounder_no_jaw_dict[viseme_list[i_next]])\n",
    "            viseme_intervals_final.append(viseme_interval)\n",
    "        increment = 2\n",
    "        \n",
    "    elif viseme_list[i] in jali_sets.lip_heavy:\n",
    "        # if the viseme is a lip-heavy viseme, the it is voice simutaneously as nearby labial dental and bilabials \n",
    "        current_interval = viseme_intervals[i] \n",
    "        if not get_kth_neighbour(viseme_list, i, -1) is None:\n",
    "            if current_interval[0][0] <= viseme_intervals[i-1][-1][0] - lyric.dt and viseme_intervals[i-1][-1][0] in jali_sets.lip_rounder:\n",
    "                current_interval[0][0] = viseme_intervals[i-1][0][0]\n",
    "                current_interval[1][0] = viseme_intervals[i-1][1][0]\n",
    "        if not get_kth_neighbour(viseme_list, i, +1) is None:\n",
    "            if current_interval[-1][0] <= viseme_intervals[i+1][0][0] - lyric.dt and viseme_intervals[i+1][-1][0] in jali_sets.lip_rounder:\n",
    "                current_interval[2][0] = viseme_intervals[i+1][0][0]\n",
    "                current_interval[3][0] = viseme_intervals[i+1][1][0]\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(current_interval)\n",
    "        if viseme_list[i] in jali_sets.lip_rounder:\n",
    "            viseme_list_final.append(jali_sets.lip_rounder_no_jaw_dict[viseme_list[i]])\n",
    "            viseme_intervals_final.append(current_interval)\n",
    "        pure_phoneme_list_final.append(pure_phoneme_list[i])\n",
    "    else:\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(viseme_intervals[i])\n",
    "        if viseme_list[i] in jali_sets.lip_rounder:\n",
    "            viseme_list_final.append(jali_sets.lip_rounder_no_jaw_dict[viseme_list[i]])\n",
    "            viseme_intervals_final.append(viseme_intervals[i])\n",
    "        pure_phoneme_list_final.append(pure_phoneme_list[i])\n",
    "    i = i + increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cause Visemes to overlap between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(viseme_list_final)-1):\n",
    "    # re-order all the clips so that \n",
    "    if pure_phoneme_list_final[i] in cmu_sets.vowels:\n",
    "        next_vowel = -1\n",
    "        for si in range(i+1, len(viseme_list_final)):\n",
    "            if pure_phoneme_list_final[si] in cmu_sets.vowels:\n",
    "                next_vowel = si\n",
    "                break\n",
    "        if next_vowel > -1:\n",
    "            interval_i = viseme_intervals_final[i]\n",
    "            interval_i.sort(key=lambda x:x[0])\n",
    "            interval_next = viseme_intervals_final[next_vowel]\n",
    "            interval_next.sort(key=lambda x:x[0])\n",
    "            if interval_i[-1][0] <= interval_next[0][0]:\n",
    "                # there is no overlap \n",
    "                interval_i[-1][1] = interval_i[-2][1]/2\n",
    "                interval_i.append([interval_next[0][0], interval_i[-1][1] * 0.75])\n",
    "                interval_i.append([interval_next[1][0], 0])\n",
    "                viseme_intervals_final[i] = interval_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vowel modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel2Cardinal5 = {\"Ah_pointer\":0, \"Aa_pointer\":1, \"Eh_pointer\":1, \"Ee_pointer\":2, \n",
    "                 \"Ih_pointer\":2, \"Oo_pointer\":3, \"Oh_pointer\":3, \"Uh_pointer\":0, \n",
    "                  \"U_pointer\":4, \"Eu_pointer\":4}\n",
    "vowel2Cardinal3 = {\"Ah_pointer\":0, \"Aa_pointer\":1, \"Eh_pointer\":1, \"Ee_pointer\":1, \n",
    "                 \"Ih_pointer\":1, \"Oo_pointer\":2, \"Oh_pointer\":2, \"Uh_pointer\":0, \n",
    "                  \"U_pointer\":2, \"Eu_pointer\":2}\n",
    "\n",
    "control_direction_matrix_coarse = {0:{1:[\"Dimple\", \"Dimple\", [0, 9]], 2:[\"Pucker\", \"Pucker\", [0, 4]]},\n",
    "                                  1:{0:[\"Pucker\", \"Pucker\", [0, 3]], 2:[\"Pucker\", \"Pucker\", [0, 4]]},\n",
    "                                  2:{0:[\"self\", \"Lip Pucker\", [0, -3]], 1:[\"self\", \"Lip Pucker\", [0, -6],\n",
    "                                                                          \"Dimple\", \"Dimple\", [0, 9]]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "viseme_list, viseme_interval = [viseme_list_final, viseme_intervals_final]\n",
    "\n",
    "modification_ctrl_pts = []\n",
    "modification_sliders = []\n",
    "# iterate through the vowels in the list\n",
    "dt = 0.01\n",
    "for i in range(0, len(viseme_list)):\n",
    "    if viseme_list[i] in VOWELS_SLIDERS_JALI and viseme_interval[i][-2][0] - viseme_interval[i][1][0] > 0.20:\n",
    "        ##################################################################\n",
    "        ###################### get the audio signal ######################\n",
    "        ##################################################################\n",
    "        vowel_mod_out, vowel_mod_out_coarse = vowel_mod(lyric.sound_arr_interp(np.arange(viseme_interval[i][1][0], \n",
    "                                                                                        min(viseme_interval[i][-1][0], lyric.snd.xs()[-1]), 1.0/44100.0)))\n",
    "#         print([\"A\", \"Stretcher\", \"Rounder\", \"Silence\"][vowel2Cardinal3[viseme_list[i]]])\n",
    "        xs = np.linspace(viseme_interval[i][1][0], min(viseme_interval[i][-1][0], lyric.snd.xs()[-1]), vowel_mod_out_coarse.shape[0])\n",
    "        coarse_vowel_sounds_like_interp = interp1d(xs, vowel_mod_out_coarse, axis=0)\n",
    "        \n",
    "#         for i in range(0, 6):\n",
    "#             plt.plot(vowel_mod_out[:, i], label=\"aeiouX\"[i])\n",
    "#         # plt.plot(np.argmax(out[0, :], axis=1))\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "        # what the original sound was\n",
    "        original_vowel_shape = vowel2Cardinal3[viseme_list[i]]\n",
    "        only_peaks = np.where(vowel_mod_out_coarse > 0.6, vowel_mod_out_coarse, original_vowel_shape)\n",
    "        vowel_sounds_like = np.argmax(only_peaks, axis=1)\n",
    "        ##################################################################\n",
    "        ### obtain the intervals of which cardinal vowels are dominant ###\n",
    "        ##################################################################\n",
    "        cardinal_list = []\n",
    "        cardinal_intervals = []\n",
    "        current_interval_start = 0\n",
    "        current_vowel = original_vowel_shape\n",
    "        for t in range(0, vowel_sounds_like.shape[0]):\n",
    "            if vowel_sounds_like[t] == current_vowel:\n",
    "                if (t == vowel_sounds_like.shape[0]-1) and current_vowel != 3:\n",
    "                    cardinal_list.append(current_vowel)\n",
    "                    cardinal_intervals.append([current_interval_start, t])\n",
    "            else:\n",
    "                if xs[t-1] - xs[current_interval_start] >= 0.1:\n",
    "                    if t > 0 and current_vowel != 3:\n",
    "                        cardinal_list.append(current_vowel)\n",
    "                        cardinal_intervals.append([current_interval_start, t-1])\n",
    "                    current_interval_start = t\n",
    "                    current_vowel = vowel_sounds_like[t]\n",
    "        ###########################################################################\n",
    "        ######### optionally additional smoothing are added to this here ##########      \n",
    "        ###########################################################################\n",
    "        cardinal_list_new = []\n",
    "        cardinal_intervals_new = []\n",
    "        j = 0\n",
    "        while j < len(cardinal_list):\n",
    "            step = 1\n",
    "            if j == len(cardinal_list) - 1:\n",
    "                cardinal_list_new.append(cardinal_list[j])\n",
    "                cardinal_intervals_new.append(cardinal_intervals[j])\n",
    "            elif cardinal_list[j] == cardinal_list[j+1] and xs[cardinal_intervals[j+1][0]] - xs[cardinal_intervals[j][1]] <= spike_width:\n",
    "                cardinal_list_new.append(cardinal_list[j])\n",
    "                cardinal_intervals_new.append([cardinal_intervals[j][0], cardinal_intervals[j+1][1]])\n",
    "                step = 2\n",
    "            elif j < len(cardinal_list) - 2:\n",
    "                if (cardinal_list[j] == cardinal_list[j+2] and xs[cardinal_intervals[j+2][0]] - xs[cardinal_intervals[j][1]] <= spike_width \n",
    "                    and cardinal_list[j+1] == original_vowel_shape):\n",
    "                    cardinal_list_new.append(cardinal_list[j])\n",
    "                    cardinal_intervals_new.append([cardinal_intervals[j][0], cardinal_intervals[j+2][1]])\n",
    "                    step = 3\n",
    "            else:\n",
    "                cardinal_list_new.append(cardinal_list[j])\n",
    "                cardinal_intervals_new.append(cardinal_intervals[j])\n",
    "            j = j + step\n",
    "        cardinal_list = cardinal_list_new\n",
    "        cardinal_intervals = cardinal_intervals_new\n",
    "#         print(cardinal_list, cardinal_intervals)\n",
    "        # now set pucker/stretch values based on the detected sound\n",
    "        ctrl_pts = []\n",
    "        for c in range(0, len(cardinal_list)):\n",
    "            if original_vowel_shape == cardinal_list[c] or cardinal_list[c] == 3:\n",
    "                continue\n",
    "            else:\n",
    "                max_prob = coarse_vowel_sounds_like_interp(xs[cardinal_intervals[c][0]:cardinal_intervals[c][1]+1])[:, cardinal_list[c]].max()\n",
    "                \n",
    "#                 print(original_vowel_shape, cardinal_list[c], max_prob)\n",
    "                slider_ct_pts = control_direction_matrix_coarse[original_vowel_shape][cardinal_list[c]]\n",
    "                for s in range(0, int(len(slider_ct_pts)/3)):\n",
    "                    # get the name and attribute of the slider\n",
    "                    slider_name = slider_ct_pts[0 + 3*s]\n",
    "                    if slider_name == \"self\":\n",
    "                        slider_name = viseme_list[i]\n",
    "                    slider_attribute = slider_ct_pts[1 + 3*s]\n",
    "                    \n",
    "                    # add a starting keyframe and ending keyframe\n",
    "                    modification_sliders.append([slider_name, slider_attribute])\n",
    "                    # the start of this curve should be earlier, e.g. at 75% of the previous interval\n",
    "                    # however, if the detected modification is at the very beginning of the vowel, then it is\n",
    "                    # applied the same time as the beginning of the vowel\n",
    "                    same_start = False\n",
    "                    if c == 0: \n",
    "#                         start_candidate = xs[cardinal_intervals[c][0]]-0.14\n",
    "#                         start = max(start_candidate, viseme_interval[i][0][0])\n",
    "                        start = viseme_interval[i][0][0]\n",
    "                        same_start = True\n",
    "                    else:\n",
    "                        start_candidate = (xs[cardinal_intervals[c-1][1]] - xs[cardinal_intervals[c-1][0]]) * 0.6 + xs[cardinal_intervals[c-1][0]]\n",
    "                        start_candidate = min(start_candidate, xs[cardinal_intervals[c-1][1]]-0.12)\n",
    "                        start = max(start_candidate, xs[cardinal_intervals[c-1][0]])\n",
    "                        if np.abs(start - viseme_interval[i][1][0]) <= 0.2:\n",
    "                            start = viseme_interval[i][0][0]\n",
    "                            same_start = True\n",
    "                    ctrl_pts.append([start, 0])\n",
    "                    # add the end point of the curve. This would either at the end of the \n",
    "                    # prediction interval, or at the end of the vowel. \n",
    "#                     modification_sliders.append([slider_name, slider_attribute])\n",
    "                    if c == len(cardinal_list) - 1:\n",
    "                        ctrl_pts.append([xs[cardinal_intervals[c][1]], 0])\n",
    "                    else:\n",
    "                        ctrl_pts.append([viseme_interval[i][-1][0], 0])        \n",
    "                    # add the peaks in the middle with the decay\n",
    "                    slider_range = slider_ct_pts[2 + 3*s]\n",
    "#                     modification_sliders.append([slider_name, slider_attribute])\n",
    "                    # with some anticipation \n",
    "                    if same_start:\n",
    "                        ctrl_pts.append([viseme_interval[i][1][0], max_prob * (slider_range[1])])\n",
    "                    else:\n",
    "                        ctrl_pts.append([xs[cardinal_intervals[c][0]]-0.13, max_prob * (slider_range[1])])\n",
    "#                     modification_sliders.append([slider_name, slider_attribute])\n",
    "                    end_p75 = (xs[cardinal_intervals[c][1]] - xs[cardinal_intervals[c][0]]) * 0.75 + xs[cardinal_intervals[c][0]]\n",
    "                    ctrl_pts.append([end_p75, max_prob * (slider_range[1]) * 0.75])\n",
    "                    modification_ctrl_pts.append(ctrl_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 smooth out un-intended artifacts due to unexpected overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "[[-0.011624999999999996, 0], [0.108375, 8], [0.336375, 6.0], [0.532375, 0]]\n",
      "[[0.628375, 0], [0.748375, 6], [0.8553749999999999, 4.5], [1.000375, 0]]\n",
      "==========\n",
      "[[0.628375, 0], [0.748375, 6], [0.8553749999999999, 4.5], [1.000375, 0]]\n",
      "[[0.9963750000000001, 0], [1.1163750000000001, 6], [1.351375, 4.5], [1.496375, 0]]\n",
      "==========\n",
      "[[0.708375, 0], [0.828375, 7], [1.044375, 5.25], [1.2363750000000002, 0]]\n",
      "[[1.1883750000000002, 0], [1.308375, 7], [1.6723750000000002, 5.25], [1.8283750000000003, 0]]\n",
      "==========\n",
      "[[0.708375, 0], [0.828375, 7], [1.044375, 5.25], [1.308375, 7], [1.6723750000000002, 5.25], [1.8283750000000003, 0]]\n",
      "[[1.7003750000000002, 0], [1.820375, 6], [1.895375, 4.5], [2.040375, 0]]\n",
      "==========\n",
      "[[0.708375, 0], [0.828375, 7], [1.044375, 5.25], [1.308375, 7], [1.6723750000000002, 5.25], [1.820375, 6], [1.895375, 4.5], [2.040375, 0]]\n",
      "[[1.7803750000000003, 0], [1.9003750000000001, 7], [1.975375, 5.25], [2.120375, 0]]\n",
      "==========\n",
      "[[1.5483750000000003, 0], [1.7083750000000002, 10], [1.792375, 7.5], [1.980375, 0]]\n",
      "[[2.068375, 0], [2.188375, 7], [2.356375, 5.25], [2.532375, 0]]\n",
      "==========\n",
      "[[1.5483750000000003, 0], [1.7083750000000002, 10], [1.792375, 7.5], [1.980375, 0]]\n",
      "[[2.292375, 0], [2.412375, 6], [2.487375, 4.5], [2.632375, 0]]\n",
      "==========\n",
      "[[2.068375, 0], [2.188375, 7], [2.356375, 5.25], [2.532375, 0]]\n",
      "[[2.388375, 0], [2.508375, 7], [2.604375, 5.25], [2.7563750000000002, 0]]\n",
      "==========\n",
      "[[2.324375, 0], [2.444375, 6], [2.519375, 4.5], [2.664375, 0]]\n",
      "[[2.532375, 0], [2.652375, 6], [2.7483750000000002, 4.5], [2.9003750000000004, 0]]\n",
      "==========\n",
      "[[2.292375, 0], [2.412375, 6], [2.487375, 4.5], [2.632375, 0]]\n",
      "[[2.740375, 0], [2.860375, 6], [2.980375, 4.5], [3.140375, 0]]\n",
      "==========\n",
      "[[1.5483750000000003, 0], [1.7083750000000002, 10], [1.792375, 7.5], [1.980375, 0]]\n",
      "[[2.860375, 0], [3.020375, 6], [3.095375, 4.5], [3.2803750000000003, 0]]\n",
      "==========\n",
      "[[2.324375, 0], [2.444375, 6], [2.519375, 4.5], [2.652375, 6], [2.7483750000000002, 4.5], [2.9003750000000004, 0]]\n",
      "[[2.932375, 0], [3.052375, 6], [3.1273750000000002, 4.5], [3.2723750000000003, 0]]\n",
      "==========\n",
      "[[2.324375, 0], [2.444375, 6], [2.519375, 4.5], [2.652375, 6], [2.7483750000000002, 4.5], [2.9003750000000004, 0]]\n",
      "[[2.980375, 0], [3.100375, 7], [3.1753750000000003, 5.25], [3.3203750000000003, 0]]\n",
      "==========\n",
      "[[2.740375, 0], [2.860375, 6], [2.980375, 4.5], [3.140375, 0]]\n",
      "[[3.0286460917772406, 0], [3.1486460917772408, 6], [3.223646091777241, 4.5], [3.368646091777241, 0]]\n",
      "==========\n",
      "[[2.660375, 0], [2.7803750000000003, 7], [2.8553750000000004, 5.25], [3.0003750000000005, 0]]\n",
      "[[3.6007875631121014, 0], [3.7207875631121015, 7], [3.8682361929927964, 5.25], [4.037385736286361, 0]]\n",
      "==========\n",
      "[[2.980375, 0], [3.100375, 7], [3.1753750000000003, 5.25], [3.3203750000000003, 0]]\n",
      "[[3.797385736286361, 0], [3.917385736286361, 6], [4.02814457824479, 4.5], [4.1731445782447905, 0]]\n",
      "==========\n",
      "[[2.932375, 0], [3.052375, 6], [3.1273750000000002, 4.5], [3.2723750000000003, 0]]\n",
      "[[5.008536979289582, 0], [5.128536979289582, 6], [5.2035369792895825, 4.5], [5.348536979289582, 0]]\n",
      "==========\n",
      "[[2.980375, 0], [3.100375, 7], [3.1753750000000003, 5.25], [3.3203750000000003, 0]]\n",
      "[[5.044375, 0], [5.620413601178936, 0]]\n",
      "==========\n",
      "[[2.860375, 0], [3.020375, 6], [3.095375, 4.5], [3.2803750000000003, 0]]\n",
      "[[5.369786935644788, 0], [5.529786935644788, 6], [5.604786935644787, 4.5], [5.789786935644788, 0]]\n",
      "==========\n",
      "[[5.044375, 0], [5.620413601178936, 0]]\n",
      "[[5.509400852528983, 0], [5.669400852528982, 9.673776010550931], [5.933400852528976, 9.673776010550931], [6.169400852528971, 9.871482376140698], [6.316433917869783, 0]]\n",
      "==========\n",
      "[[3.0644049337356694, 0], [3.1844049337356695, 9.50006318690552], [3.4164049337356643, 9.529082619467369], [3.4744049337356633, 9.556455898754313], [3.6453910338392586, 0]]\n",
      "[[6.244375, 0], [6.364375, 9.9], [6.5083749999999965, 9.896644230304439], [6.974374999999987, 4.117889652524401], [7.138653790401756, 0]]\n",
      "==========\n",
      "[[5.369786935644788, 0], [5.529786935644788, 6], [5.604786935644787, 4.5], [5.789786935644788, 0]]\n",
      "[[6.898653790401756, 0], [7.460375, 0]]\n",
      "==========\n",
      "[[5.369786935644788, 0], [5.529786935644788, 6], [5.604786935644787, 4.5], [5.789786935644788, 0]]\n",
      "[[7.212375, 0], [7.372375, 10], [7.4563749999999995, 7.5], [7.644375, 0]]\n",
      "==========\n",
      "[[3.797385736286361, 0], [3.917385736286361, 6], [4.02814457824479, 4.5], [4.1731445782447905, 0]]\n",
      "[[7.364375, 0], [7.484375, 6], [7.559374999999999, 4.5], [7.704375, 0]]\n",
      "==========\n",
      "[[6.244375, 0], [6.364375, 9.9], [6.5083749999999965, 9.896644230304439], [6.974374999999987, 4.117889652524401], [7.138653790401756, 0]]\n",
      "[[7.428375, 0], [7.558375, 9.676298713043305], [7.726374999999996, 9.676298713043305], [7.768374999999995, 9.52416729446686], [7.988375, 0]]\n",
      "==========\n",
      "[[5.380413601178936, 0], [5.500413601178936, 6], [5.575413601178935, 4.5], [5.720413601178936, 0]]\n",
      "[[7.748374999999999, 0], [7.8683749999999995, 6], [7.943375, 4.5], [8.088375, 0]]\n",
      "==========\n",
      "[[7.364375, 0], [7.484375, 6], [7.559374999999999, 4.5], [7.704375, 0]]\n",
      "[[7.780374999999999, 0], [7.9003749999999995, 6], [7.975375, 4.5], [8.120375, 0]]\n",
      "==========\n",
      "[[3.6007875631121014, 0], [3.7207875631121015, 7], [3.8682361929927964, 5.25], [4.037385736286361, 0]]\n",
      "[[7.828374999999999, 0], [7.9483749999999995, 7], [8.068375, 5.25], [8.228375, 0]]\n",
      "==========\n",
      "[[7.780374999999999, 0], [7.9003749999999995, 6], [7.975375, 4.5], [8.120375, 0]]\n",
      "[[7.9883750000000004, 0], [8.108375, 6], [8.183375, 4.5], [8.328375, 0]]\n",
      "==========\n",
      "[[7.828374999999999, 0], [7.9483749999999995, 7], [8.068375, 5.25], [8.228375, 0]]\n",
      "[[8.068375000000001, 0], [8.188375, 7], [8.263375, 5.25], [8.408375, 0]]\n",
      "==========\n",
      "[[7.828374999999999, 0], [7.9483749999999995, 7], [8.068375, 5.25], [8.188375, 7], [8.263375, 5.25], [8.408375, 0]]\n",
      "[[8.148375000000001, 0], [8.268375, 6], [8.343375, 4.5], [8.488375, 0]]\n",
      "==========\n",
      "[[1.876375, 0], [1.996375, 10], [2.140375, 7.5], [2.3083750000000003, 0]]\n",
      "[[8.180375000000002, 0], [8.300375, 10], [8.444375, 7.5], [8.612375, 0]]\n",
      "==========\n",
      "[[7.428375, 0], [7.558375, 9.676298713043305], [7.726374999999996, 9.676298713043305], [7.768374999999995, 9.52416729446686], [7.988375, 0]]\n",
      "[[8.372375000000002, 0], [8.502375, 4.0], [8.822374999999994, 4.502111310325497], [8.932374999999992, 9.803331903392145], [9.060375, 0]]\n",
      "==========\n",
      "[[8.372375000000002, 0], [8.502375, 4.0], [8.822374999999994, 4.502111310325497], [8.932374999999992, 9.803331903392145], [9.060375, 0]]\n",
      "[[8.836375000000002, 0], [8.956375000000001, 6], [9.342374999999995, 4.0], [9.434374999999994, 9.842268089789274], [9.556375, 4.921134044894637], [9.812375000000001, 3.690850533670978], [10.042374999999998, 0]]\n",
      "==========\n",
      "[[7.748374999999999, 0], [7.8683749999999995, 6], [7.943375, 4.5], [8.088375, 0]]\n",
      "[[9.316375, 0], [9.436375, 6], [9.511375, 4.5], [9.656374999999999, 0]]\n",
      "==========\n",
      "[[7.828374999999999, 0], [7.9483749999999995, 7], [8.068375, 5.25], [8.188375, 7], [8.263375, 5.25], [8.268375, 6], [8.343375, 4.5], [8.488375, 0]]\n",
      "[[9.348375, 0], [9.468375, 6], [9.543375, 4.5], [9.688374999999999, 0]]\n",
      "==========\n",
      "[[9.316375, 0], [9.436375, 6], [9.511375, 4.5], [9.656374999999999, 0]]\n",
      "[[9.768564493795614, 0], [9.888564493795613, 6], [9.963564493795612, 4.5], [10.108564493795612, 0]]\n",
      "==========\n",
      "[[7.828374999999999, 0], [7.9483749999999995, 7], [8.068375, 5.25], [8.188375, 7], [8.263375, 5.25], [8.268375, 6], [8.343375, 4.5], [8.488375, 0]]\n",
      "[[9.812375000000001, 0], [10.042374999999998, 9.898041813709664], [10.194374999999996, 9.881815631215973], [10.232374999999994, 9.839206631699943], [10.404375, 0]]\n",
      "==========\n",
      "[[9.812375000000001, 0], [10.042374999999998, 9.898041813709664], [10.194374999999996, 9.881815631215973], [10.232374999999994, 9.839206631699943], [10.404375, 0]]\n",
      "[[10.276375000000002, 0], [10.396375, 7], [10.576375, 5.25], [10.756375, 0]]\n",
      "==========\n",
      "[[9.348375, 0], [9.468375, 6], [9.543375, 4.5], [9.688374999999999, 0]]\n",
      "[[10.516375000000002, 0], [10.636375000000001, 6], [10.756375000000002, 4.5], [10.916375, 0]]\n",
      "==========\n",
      "[[6.898653790401756, 0], [7.460375, 0]]\n",
      "[[10.692375000000002, 0], [10.812375000000001, 7], [10.944375, 5.25], [11.108375, 0]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_18176/3869705192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurrent_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mviseme_intervals_final_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_slider_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mviseme_list_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurrent_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mprev_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurrent_interval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mviseme_intervals_final_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_slider_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mviseme_list_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# pass 3\n",
    "# set this up\n",
    "prev_slider_dict = {}\n",
    "for i in range(0, len(list(jali_sets.vocabs))):\n",
    "    prev_slider_dict[list(jali_sets.vocabs)[i]] = -1\n",
    "viseme_list_final_final = []\n",
    "viseme_intervals_final_final = []\n",
    "i = 0  \n",
    "while i < len(viseme_list_final):\n",
    "    increment = 1\n",
    "    prev_viseme = viseme_list_final[i]\n",
    "    # if the previous instance of the current viseme is not -1\n",
    "    if prev_slider_dict[viseme_list_final[i]] != -1:\n",
    "        current_interval = viseme_intervals_final[i]\n",
    "        prev_interval = viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]]\n",
    "        print(\"==========\")\n",
    "        print(prev_interval)\n",
    "        print(current_interval)\n",
    "        \n",
    "        if (current_interval[1][0] >= prev_interval[-2][0] and current_interval[0][0] <= prev_interval[-1][0]):\n",
    "            interval = prev_interval[:-1] + current_interval[1:]\n",
    "            viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]] = interval\n",
    "        elif (current_interval[1][0] <= prev_interval[2][0]):\n",
    "            interval = prev_interval[0:-2] + current_interval[1:]\n",
    "            viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]] = interval\n",
    "        else:\n",
    "            viseme_list_final_final.append(viseme_list_final[i])\n",
    "            viseme_intervals_final_final.append(viseme_intervals_final[i])\n",
    "                \n",
    "    else:        \n",
    "        viseme_list_final_final.append(viseme_list_final[i])\n",
    "        viseme_intervals_final_final.append(viseme_intervals_final[i])\n",
    "        \n",
    "    prev_slider_dict[viseme_list_final[i]] = len(viseme_list_final_final) - 1\n",
    "    i = i + increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "Tha_pointer U_pointer Eh_pointer Ra_pointer SZ_pointer\n",
      "Ah_pointer FVa_pointer Eh_pointer SZ_pointer LNTDa_pointer\n"
     ]
    }
   ],
   "source": [
    "print(len(viseme_list_final))\n",
    "print(len(viseme_intervals_final))\n",
    "for i in range(0, 80):\n",
    "    if len(viseme_intervals_final[i]) == 2:\n",
    "        print(viseme_list_final[i-1], viseme_list_final[i-2], viseme_list_final[i], viseme_list_final[i+1], viseme_list_final[i+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do the same thing to vowel modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass 3\n",
    "# set this up\n",
    "prev_slider_dict = {}\n",
    "for i in range(0, len(modification_sliders)):\n",
    "    prev_slider_dict[modification_sliders[i][0] + \"_\" + modification_sliders[i][1]] = -1\n",
    "modification_sliders_final = []\n",
    "modification_ctrl_pts_final = []\n",
    "i = 0  \n",
    "while i < len(modification_sliders):\n",
    "    increment = 1\n",
    "    prev_viseme = modification_sliders[i][0] + \"_\" + modification_sliders[i][1]\n",
    "    # if the previous instance of the current viseme is not -1\n",
    "    if prev_slider_dict[prev_viseme] != -1:\n",
    "        current_interval = modification_ctrl_pts[i]\n",
    "        current_interval.sort(key=lambda x:x[0])\n",
    "        prev_interval = modification_ctrl_pts[prev_slider_dict[prev_viseme]]\n",
    "        prev_interval.sort(key=lambda x:x[0])\n",
    "        # if there is a weird insertion\n",
    "        if (current_interval[1][0] >= prev_interval[2][0] and current_interval[0][0] <= prev_interval[3][0]):\n",
    "            interval = prev_interval[:-1] + current_interval[1:]\n",
    "            modification_ctrl_pts_final[prev_slider_dict[prev_viseme]] = interval\n",
    "        elif (current_interval[1][0] <= prev_interval[2][0]):\n",
    "            interval = prev_interval[0:-2] + current_interval[1:]\n",
    "            modification_ctrl_pts_final[prev_slider_dict[prev_viseme]] = interval\n",
    "        else:\n",
    "            modification_sliders_final.append(modification_sliders[i])\n",
    "            modification_ctrl_pts_final.append(modification_ctrl_pts[i])\n",
    "                \n",
    "    else:        \n",
    "        modification_sliders_final.append(modification_sliders[i])\n",
    "        modification_ctrl_pts_final.append(modification_ctrl_pts[i])\n",
    "        \n",
    "    prev_slider_dict[modification_sliders[i][0] + \"_\" + modification_sliders[i][1]] = len(modification_sliders_final) - 1\n",
    "    i = i + increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_ctrl_pts = []\n",
    "modification_sliders = []\n",
    "for i in range(0, len(modification_ctrl_pts_final)):\n",
    "    for j in range(0, len(modification_ctrl_pts_final[i])):\n",
    "        modification_sliders.append(modification_sliders_final[i])\n",
    "        modification_ctrl_pts.append(modification_ctrl_pts_final[i][j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.45\n",
      "13.378333333333332\n",
      "13.42\n",
      "13.306666666666665\n",
      "13.41\n",
      "13.234999999999998\n",
      "13.4\n",
      "13.163333333333332\n",
      "13.25\n",
      "13.091666666666665\n",
      "13.15\n",
      "13.019999999999998\n",
      "13.12\n",
      "12.94833333333333\n",
      "13.06\n",
      "12.876666666666663\n",
      "13.01\n",
      "12.804999999999998\n",
      "12.95\n",
      "12.73333333333333\n",
      "12.86\n",
      "12.661666666666664\n",
      "12.79\n",
      "12.589999999999996\n",
      "12.77\n",
      "12.518333333333329\n",
      "12.73\n",
      "12.446666666666662\n",
      "12.709999999999999\n",
      "12.374999999999995\n",
      "12.67\n",
      "12.30333333333333\n",
      "12.65\n",
      "12.231666666666662\n",
      "12.59\n",
      "12.159999999999995\n",
      "12.58\n",
      "12.088333333333328\n",
      "12.55\n",
      "12.01666666666666\n",
      "12.51\n",
      "11.944999999999995\n",
      "12.42\n",
      "11.873333333333328\n",
      "12.39\n",
      "11.80166666666666\n",
      "12.2\n",
      "11.729999999999993\n",
      "12.11\n",
      "11.658333333333326\n",
      "12.06\n",
      "11.586666666666659\n",
      "11.79\n",
      "11.514999999999992\n",
      "11.56\n",
      "11.443333333333326\n",
      "11.549999999999999\n",
      "11.371666666666659\n",
      "11.47\n",
      "11.299999999999992\n",
      "11.44\n",
      "11.228333333333325\n",
      "11.4\n",
      "11.156666666666657\n",
      "11.35\n",
      "11.084999999999992\n",
      "11.31\n",
      "11.013333333333323\n",
      "11.26\n",
      "10.941666666666658\n",
      "11.25\n",
      "10.86999999999999\n",
      "11.16\n",
      "10.798333333333323\n",
      "10.97\n",
      "10.726666666666656\n",
      "10.86\n",
      "10.654999999999989\n",
      "10.73\n",
      "10.583333333333323\n",
      "10.7\n",
      "10.511666666666656\n",
      "10.629999999999999\n",
      "10.439999999999989\n",
      "10.62\n",
      "10.368333333333322\n",
      "10.54\n",
      "10.296666666666654\n",
      "10.24\n",
      "10.224999999999989\n",
      "10.22\n",
      "10.15333333333332\n",
      "10.209999999999999\n",
      "10.081666666666655\n",
      "10.2\n",
      "10.009999999999987\n",
      "10.06\n",
      "9.93833333333332\n",
      "9.48\n",
      "9.866666666666653\n"
     ]
    }
   ],
   "source": [
    "lyric.compute_self_vibrato_intervals()\n",
    "vib_ctrl_pts = []\n",
    "# for k in lyric.vibrato_intervals:\n",
    "#     if len(k) > 0:\n",
    "#         for m in k:\n",
    "#             vib_ctrl_pts.append(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create additional overlap between vowels so it's smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, len(viseme_list)):\n",
    "#     # re-order all the clips so that \n",
    "#     if pure_phoneme_list[i] in cmu_sets.vowels:\n",
    "#         prev_vowel = -1\n",
    "#         for si in range(i-1, -1, -1):\n",
    "#             if pure_phoneme_list[si] in cmu_sets.vowels:\n",
    "#                 prev_vowel = si\n",
    "#                 break\n",
    "#         if prev_vowel > -1 and viseme_list[i] != viseme_list[prev_vowel]:\n",
    "#             interval_i = viseme_intervals[i]\n",
    "#             interval_i.sort(key=lambda x:x[0])\n",
    "#             interval_prev = viseme_intervals[prev_vowel]\n",
    "#             interval_prev.sort(key=lambda x:x[0])\n",
    "#             if interval_i[0][0] <= interval_prev[-1][0]:\n",
    "#                 # there is an overlap\n",
    "#                 interval_prev[-2][0] = interval_i[1][0]\n",
    "#                 interval_prev[-1][0] = interval_i[1][0] + 0.12\n",
    "#                 viseme_intervals[prev_vowel] = interval_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\"viseme\":[viseme_list_final_final, viseme_intervals_final_final],\n",
    "#         \"brow\":[brow_movement, brow_ctrl_points, finer_brow_raise_ctrl_points, finer_brow_furrow_ctrl_points],\n",
    "#         \"blink\":[eye_movement, eye_ctrl_points],\n",
    "        \"vowel_mod\": [modification_sliders, modification_ctrl_pts],\n",
    "        \"jaw\":[[0, 6]],\n",
    "        \"lip\":[[0, 6]], \n",
    "        \"vib\":vib_ctrl_pts}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template + \"_\" + output_template + '.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaligaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
