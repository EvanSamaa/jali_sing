{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.sound_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "VOWELS = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW', \"ER\", \"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e9cfeb394a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"E:/Structured_data/rolling_in_the_deep_adele\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_lyric_alignment_textgrids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/jali_sing/util/sound_processing.py\u001b[0m in \u001b[0;36mcombine_lyric_alignment_textgrids\u001b[0;34m(dir, file_name_template)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;31m# get all the textgrid_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0msub_audio_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextgrid_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\S*TextGrid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextgrid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'"
     ]
    }
   ],
   "source": [
    "dir = \"E:/Structured_data/rolling_in_the_deep_adele\"\n",
    "file_name_template = \"audio\"\n",
    "lyric = combine_lyric_alignment_textgrids(dir, file_name_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric.compute_self_pitch_intervals()\n",
    "lyric.compute_self_vibrato_intervals()\n",
    "lyric.compute_self_singing_style_intervals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation_ctrl_pts(start, end, value, sustain=1, decay = 0.75, onset=0.1, offset=0):\n",
    "    interval = []\n",
    "    interval.append([start-onset, 0])\n",
    "    # second point is when the belting starts \n",
    "    interval.append([start, 1 * value])\n",
    "    # third point emphasizes decay, it happens 75% down the interval\n",
    "    if sustain < 1:\n",
    "        interval.append([start + sustain * (end - start), decay * value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    elif sustain == 1:\n",
    "        interval.append([end, value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute coarse intervals from fine ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = lyric.voice_quality_intervals\n",
    "traits = lyric.voice_quality_lists\n",
    "def compute_coarse_intervals(traits, interval):\n",
    "    new_intervals = []\n",
    "    new_traits = []\n",
    "    for i in range(0, len(intervals)):\n",
    "        new_interval = []\n",
    "        new_trait = []\n",
    "        interval = intervals[i]\n",
    "        trait = traits[i]\n",
    "        if len(trait) > 1:\n",
    "            prev_trait = trait[0]\n",
    "            prev_index = 0\n",
    "            for k in range(1, len(trait)):\n",
    "                if trait[k] == prev_trait and k == len(trait)-1:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k][1]])\n",
    "                elif trait[k] == prev_trait:\n",
    "                    continue\n",
    "                elif trait[k] != prev_trait:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k-1][1]])\n",
    "                    prev_trait = trait[k]\n",
    "            new_traits.append(new_trait)\n",
    "            new_intervals.append(new_interval)\n",
    "        else:\n",
    "            new_traits.append(trait)\n",
    "            new_intervals.append(interval)\n",
    "    return new_traits, new_intervals\n",
    "new_traits, new_interval = compute_coarse_intervals(traits, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Eye Brow Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brow_movement = []\n",
    "brow_intervals = []\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    has_belt_pitch_interval_id = -1\n",
    "    has_head_pitch_interval_id = -1\n",
    "    has_belt_word_id = -1\n",
    "    has_head_word_id = -1\n",
    "    for phone_id in sentence:\n",
    "        phone = lyric.phoneme_list[phone_id]\n",
    "        voice_qualities = lyric.coarse_voice_quality_lists[phone_id]\n",
    "        voice_intervals = lyric.coarse_voice_quality_intervals[phone_id]\n",
    "        # look to see if there are any parts that has belting\n",
    "        for voice_quality_id in range(0, len(voice_qualities)):\n",
    "            if (voice_qualities[voice_quality_id] == \"belt\" and has_belt_word_id < 0 and \n",
    "                voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.4):\n",
    "                has_belt_word_id = phone_id\n",
    "                has_belt_pitch_interval_id = voice_quality_id\n",
    "            elif (voice_qualities[voice_quality_id] == \"head\" and has_belt_word_id >= 0 and has_head_word_id < 0 and \n",
    "                 voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.2):\n",
    "                has_head_word_id = phone_id\n",
    "                has_head_pitch_interval_id = voice_quality_id\n",
    "            if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "                break\n",
    "    if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "        interval = []\n",
    "        interval.append(lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0])\n",
    "        interval.append(lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0]-0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_intervals.append(interval)\n",
    "        \n",
    "        brow_movement.append(\"raise\")\n",
    "        interval = []\n",
    "        interval.append(lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0])\n",
    "        interval.append(lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][1])\n",
    "        brow_intervals.append(interval)\n",
    "    elif has_belt_word_id > 0 and has_head_word_id < 0:\n",
    "        interval = []\n",
    "        interval.append(lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0])\n",
    "        interval.append(lyric.phoneme_intervals[sentence[-1]][1]-0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_intervals.append(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Write the output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\"brow\":[brow_movement, brow_intervals]}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Compute Eye Brow Movements version 2\n",
    "This will change the data format for saving, and use more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is mostly for word level i.e. long vowels in a word. It would also be necessary to explore\n",
    "# sentence level expressions (possibly from studying another song)\n",
    "# this will have array of either 3 or 4 control points. \n",
    "# i.e. [[[ctrl_pt_1]...[ctrl_pt_k]], [[ctrl_pt_1]...[ctrl_pt_k]]] \n",
    "brow_movement = []\n",
    "brow_ctrl_points = []\n",
    "eye_movement = []\n",
    "eye_ctrl_points = []\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    has_belt_pitch_interval_id = -1\n",
    "    has_belt_word_id = -1\n",
    "    has_head_pitch_interval_id = -1\n",
    "    has_head_word_id = -1\n",
    "    only_has_pitch_interval_id = -1\n",
    "    only_has_head_word_id = -1\n",
    "    \n",
    "    for phone_id in sentence:\n",
    "        phone = lyric.phoneme_list[phone_id]\n",
    "        voice_qualities = lyric.coarse_voice_quality_lists[phone_id]\n",
    "        voice_intervals = lyric.coarse_voice_quality_intervals[phone_id]\n",
    "        # look to see if there are any parts that has belting\n",
    "        for voice_quality_id in range(0, len(voice_qualities)):\n",
    "            if (voice_qualities[voice_quality_id] == \"belt\" and has_belt_word_id < 0 and \n",
    "                voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.4):\n",
    "                has_belt_word_id = phone_id\n",
    "                has_belt_pitch_interval_id = voice_quality_id\n",
    "            elif (voice_qualities[voice_quality_id] == \"head\" and has_belt_word_id >= 0 and has_head_word_id < 0 and \n",
    "                 voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.2):\n",
    "                has_head_word_id = phone_id\n",
    "                has_head_pitch_interval_id = voice_quality_id\n",
    "            if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "                break\n",
    "                \n",
    "    # do a second pass looking for brow raises\n",
    "    if has_belt_word_id < 0 and has_head_word_id < 0:\n",
    "        for phone_id in sentence:\n",
    "            phone = lyric.phoneme_list[phone_id]\n",
    "            pitch_change_interval = lyric.pitch_intervals[phone_id]\n",
    "            pitch_change_slopes = lyric.pitch_slopes[phone_id]\n",
    "            # look to see if there are any parts that has belting\n",
    "            for vi in range(0, len(pitch_change_slopes)):\n",
    "                if pitch_change_slopes[vi] >= 100 and only_has_head_word_id < 0:\n",
    "                    only_has_head_word_id = phone_id \n",
    "                    only_has_pitch_interval_id = vi\n",
    "                    break\n",
    "            if only_has_head_word_id >= 0:\n",
    "                break\n",
    "                \n",
    "    # this section of the code deal with having head voice only segments and the eye\n",
    "    # brow raising in those \n",
    "    if only_has_head_word_id > 0:\n",
    "        value = 5\n",
    "        onset = lyric.pitch_intervals[only_has_head_word_id][0][0]\n",
    "        start = lyric.pitch_intervals[only_has_head_word_id][only_has_pitch_interval_id][0] # where belting starts\n",
    "        onset = start - onset\n",
    "        end = lyric.pitch_intervals[only_has_head_word_id][-1][1]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, onset)\n",
    "        brow_movement.append(\"raise\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "    # this section of the code deal with belting and the related physiological points of the eyes\n",
    "    if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "        # deal with the furrowing related movements\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "                                                                                             # end of belting   \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        # deal with the eyebrow raise related movements\n",
    "         \n",
    "        value = 5\n",
    "        start = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0]\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][1]        \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_ctrl_points.append(interval)\n",
    "        brow_movement.append(\"raise\")\n",
    "        \n",
    "        # deal with eye openning and closing\n",
    "        value = 10\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "        \n",
    "    elif has_belt_word_id > 0 and has_head_word_id < 0:\n",
    "        # if there is belting, but sentence do not end with the use of head voice\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0]\n",
    "        start = min(start - 0.1, lyric.voice_quality_intervals[has_belt_word_id][0][0]) \n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        value = 10\n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1 # end of this sentence if there is no next sentence\n",
    "        if i < len(sentences)-1:\n",
    "            # the end will be the first word from the next sentence if there are more than one sentence\n",
    "            start_of_next_sentence = -1\n",
    "            next_sentence = sentences[i+1]\n",
    "            for phone_id in next_sentence:\n",
    "                if lyric.phoneme_list[phone_id] in VOWELS:\n",
    "                    start_of_next_sentence = phone_id\n",
    "                    break\n",
    "            end = lyric.phoneme_intervals[start_of_next_sentence][0]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 0.05, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['furrow', 'raise', 'furrow'] [[[1.3849886621315188, 0], [1.484988662131519, 8], [3.79129404053288, 6.0], [4.5600625, 0]], [[6.464988662131519, 0], [6.594988662131519, 5], [8.64998866213152, 3.75], [9.33498866213152, 0]], [[11.16498866213152, 0], [11.26498866213152, 8], [13.69480966553288, 6.0], [14.50475, 0]]]\n"
     ]
    }
   ],
   "source": [
    "output ={\"brow\":[brow_movement, brow_ctrl_points],\n",
    "        \"blink\":[eye_movement, eye_ctrl_points]}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootleg Jali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.CMU2JALI import *\n",
    "CMU_VOCABULARY = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G',\n",
    "                  'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "                  'UW', 'V', 'W', 'Y', 'Z', 'ZH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lyric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f943f267ee7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphoneme_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mphoneme_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lyric' is not defined"
     ]
    }
   ],
   "source": [
    "phoneme_list = lyric.phoneme_list\n",
    "phoneme_interval = lyric.phoneme_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "viseme_list = []\n",
    "viseme_intervals = []\n",
    "\n",
    "prev_vowel = \"Uh\"\n",
    "next_vowel = \"\"\n",
    "# pass 1\n",
    "for i in range(0, len(phoneme_list)):\n",
    "    if phoneme_list[i] in CMU_VOCABULARY:\n",
    "        viseme_jali = CMU2VISEME[phoneme_list[i]]+\"_pointer\"\n",
    "        start = phoneme_interval[i][0]\n",
    "        end = phoneme_interval[i][1]\n",
    "        if (end - start) <= 0.4:\n",
    "            value = 5\n",
    "            sustain = 0.75\n",
    "            decay = 0.75\n",
    "        else:\n",
    "            value = 8\n",
    "            sustain = 0.9\n",
    "            decay = 0.95\n",
    "        \n",
    "        if not viseme_jali in VOWELS_JALI:\n",
    "            value = 9\n",
    "        viseme_curve = generate_animation_ctrl_pts(start, end, value, sustain=0.75, decay=decay, onset=0.12, offset=0.12)\n",
    "        viseme_list.append(viseme_jali)\n",
    "        viseme_intervals.append(viseme_curve)\n",
    "# pass 2 enforcing co-articulation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\"viseme\":[viseme_list, viseme_intervals]}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying eye brow movements using facial landmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.facial_landmarking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = [\"video.mp4\"]\n",
    "video_path = [\"E:/facial_data_analysis_videos/1\"]\n",
    "extract_landmarks_media_pipe(video_title[0],\n",
    "                         video_path[0], save_annotated_video=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jali_sing",
   "language": "python",
   "name": "jali_sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
