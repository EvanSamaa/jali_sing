{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.sound_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from util.sound_processing import *\n",
    "VOWELS = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW', \"ER\", \"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e9cfeb394a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"E:/Structured_data/rolling_in_the_deep_adele\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_lyric_alignment_textgrids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/jali_sing/util/sound_processing.py\u001b[0m in \u001b[0;36mcombine_lyric_alignment_textgrids\u001b[0;34m(dir, file_name_template)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;31m# get all the textgrid_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0msub_audio_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextgrid_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\S*TextGrid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextgrid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'"
     ]
    }
   ],
   "source": [
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep2_adale\"\n",
    "file_name_template = \"audio_vocals\"\n",
    "lyric = PraatScript_Lyric_Wrapper(os.path.join(dir, file_name_template+\".wav\"), os.path.join(dir, file_name_template+\".txt\"), \n",
    "                                  sentence_textgrids_path=[dir+\"/audio_vocals_fixed.TextGrid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep_adele2\"\n",
    "file_name_template = \"audio_vovals\"\n",
    "lyric = combine_lyric_alignment_textgrids(dir, file_name_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric.compute_self_pitch_intervals()\n",
    "lyric.compute_self_vibrato_intervals()\n",
    "lyric.compute_self_singing_style_intervals()\n",
    "lyric.write_textgrid(dir, file_name_template+\"_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation_ctrl_pts(start, end, value, sustain=1, decay = 0.75, onset=0.1, offset=0):\n",
    "    interval = []\n",
    "    interval.append([start-onset, 0])\n",
    "    # second point is when the belting starts \n",
    "    interval.append([start, 1 * value])\n",
    "    # third point emphasizes decay, it happens 75% down the interval\n",
    "    if sustain < 1:\n",
    "        interval.append([start + sustain * (end - start), decay * value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    elif sustain == 1:\n",
    "        interval.append([end, value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute coarse intervals from fine ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], ['head'], [], [], ['head'], [], ['belt'], [], [], [], ['head'], [], [], ['head'], [], [], ['belt'], [], [], [], [], ['belt'], [], ['head'], [], [], ['belt'], ['head'], [], [], ['head'], [], [], ['head'], [], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], ['head'], [], ['belt'], [], [], ['head'], [], [], [], ['belt'], [], [], [], ['belt'], ['head'], [], [], [], [], ['head'], ['head'], [], [], [], ['belt'], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], [], [], ['head'], [], [], ['head'], []]\n",
      "[[], [], [], ['head'], [], [], ['head'], [], ['belt'], [], [], [], ['head'], [], [], ['head'], [], [], ['belt', 'belt', 'belt', 'belt', 'belt', 'belt', 'belt', 'belt', 'belt'], [], [], [], [], ['belt'], [], ['head'], [], [], ['belt'], ['head'], [], [], ['head'], [], [], ['head', 'head', 'head', 'head', 'head', 'head', 'head', 'head', 'head', 'head'], [], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], ['head'], [], ['belt', 'belt', 'belt', 'belt', 'belt'], [], [], ['head'], [], [], [], ['belt'], [], [], [], ['belt', 'belt', 'belt'], ['head'], [], [], [], [], ['head'], ['head'], [], [], [], ['belt'], [], [], [], ['head'], [], [], ['head'], [], [], [], ['head'], [], [], ['head'], [], [], ['head', 'head', 'head', 'head'], []]\n"
     ]
    }
   ],
   "source": [
    "intervals = lyric.voice_quality_intervals\n",
    "traits = lyric.voice_quality_lists\n",
    "def compute_coarse_intervals(traits, interval):\n",
    "    new_intervals = []\n",
    "    new_traits = []\n",
    "    for i in range(0, len(intervals)):\n",
    "        new_interval = []\n",
    "        new_trait = []\n",
    "        interval = intervals[i]\n",
    "        trait = traits[i]\n",
    "        if len(trait) > 1:\n",
    "            prev_trait = trait[0]\n",
    "            prev_index = 0\n",
    "            for k in range(1, len(trait)):\n",
    "                if trait[k] == prev_trait and k == len(trait)-1:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k][1]])\n",
    "                elif trait[k] == prev_trait:\n",
    "                    continue\n",
    "                elif trait[k] != prev_trait:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k-1][1]])\n",
    "                    prev_trait = trait[k]\n",
    "            new_traits.append(new_trait)\n",
    "            new_intervals.append(new_interval)\n",
    "        else:\n",
    "            new_traits.append(trait)\n",
    "            new_intervals.append(interval)\n",
    "    return new_traits, new_intervals\n",
    "new_traits, new_interval = compute_coarse_intervals(traits, intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Eye Brow Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Compute Eye Brow Movements version 2\n",
    "This will change the data format for saving, and use more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is mostly for word level i.e. long vowels in a word. It would also be necessary to explore\n",
    "# sentence level expressions (possibly from studying another song)\n",
    "# this will have array of either 3 or 4 control points. \n",
    "# i.e. [[[ctrl_pt_1]...[ctrl_pt_k]], [[ctrl_pt_1]...[ctrl_pt_k]]] \n",
    "brow_movement = []\n",
    "brow_ctrl_points = []\n",
    "eye_movement = []\n",
    "eye_ctrl_points = []\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    has_belt_pitch_interval_id = -1\n",
    "    has_belt_word_id = -1\n",
    "    has_head_pitch_interval_id = -1\n",
    "    has_head_word_id = -1\n",
    "    only_has_pitch_interval_id = -1\n",
    "    only_has_head_word_id = -1\n",
    "    \n",
    "    for phone_id in sentence:\n",
    "        phone = lyric.phoneme_list[phone_id]\n",
    "        voice_qualities = lyric.coarse_voice_quality_lists[phone_id]\n",
    "        voice_intervals = lyric.coarse_voice_quality_intervals[phone_id]\n",
    "        # look to see if there are any parts that has belting\n",
    "        for voice_quality_id in range(0, len(voice_qualities)):\n",
    "            if (voice_qualities[voice_quality_id] == \"belt\" and has_belt_word_id < 0 and \n",
    "                voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.4):\n",
    "                has_belt_word_id = phone_id\n",
    "                has_belt_pitch_interval_id = voice_quality_id\n",
    "            elif (voice_qualities[voice_quality_id] == \"head\" and has_belt_word_id >= 0 and has_head_word_id < 0 and \n",
    "                 voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.2):\n",
    "                has_head_word_id = phone_id\n",
    "                has_head_pitch_interval_id = voice_quality_id\n",
    "            if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "                break\n",
    "                \n",
    "    # do a second pass looking for brow raises\n",
    "    if has_belt_word_id < 0 and has_head_word_id < 0:\n",
    "        for phone_id in sentence:\n",
    "            phone = lyric.phoneme_list[phone_id]\n",
    "            pitch_change_interval = lyric.pitch_intervals[phone_id]\n",
    "            pitch_change_slopes = lyric.pitch_slopes[phone_id]\n",
    "            # look to see if there are any parts that has belting\n",
    "            for vi in range(0, len(pitch_change_slopes)):\n",
    "                if pitch_change_slopes[vi] >= 100 and only_has_head_word_id < 0:\n",
    "                    only_has_head_word_id = phone_id \n",
    "                    only_has_pitch_interval_id = vi\n",
    "                    break\n",
    "            if only_has_head_word_id >= 0:\n",
    "                break\n",
    "                \n",
    "    # this section of the code deal with having head voice only segments and the eye\n",
    "    # brow raising in those \n",
    "    if only_has_head_word_id > 0:\n",
    "        value = 5\n",
    "        onset = lyric.pitch_intervals[only_has_head_word_id][0][0]\n",
    "        start = lyric.pitch_intervals[only_has_head_word_id][only_has_pitch_interval_id][0] # where belting starts\n",
    "        onset = start - onset\n",
    "        end = lyric.pitch_intervals[only_has_head_word_id][-1][1]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, onset)\n",
    "        brow_movement.append(\"raise\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "    # this section of the code deal with belting and the related physiological points of the eyes\n",
    "    if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "        # deal with the furrowing related movements\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "                                                                                             # end of belting   \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        # deal with the eyebrow raise related movements\n",
    "         \n",
    "        value = 5\n",
    "        start = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0]\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][1]        \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_ctrl_points.append(interval)\n",
    "        brow_movement.append(\"raise\")\n",
    "        \n",
    "        # deal with eye openning and closing\n",
    "        value = 10\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "        \n",
    "    elif has_belt_word_id > 0 and has_head_word_id < 0:\n",
    "        # if there is belting, but sentence do not end with the use of head voice\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0]\n",
    "        start = min(start - 0.1, lyric.voice_quality_intervals[has_belt_word_id][0][0]) \n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.05)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        value = 10\n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1 # end of this sentence if there is no next sentence\n",
    "        if i < len(sentences)-1:\n",
    "            # the end will be the first word from the next sentence if there are more than one sentence\n",
    "            start_of_next_sentence = -1\n",
    "            next_sentence = sentences[i+1]\n",
    "            for phone_id in next_sentence:\n",
    "                if lyric.phoneme_list[phone_id] in VOWELS:\n",
    "                    start_of_next_sentence = phone_id\n",
    "                    break\n",
    "            end = lyric.phoneme_intervals[start_of_next_sentence][0]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 1, 0.05, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "brow_movement.append(\"raise\")\n",
    "brow_movement.append(\"furrow\")\n",
    "brow_ctrl_points.append([[lyric.phoneme_intervals[-1][1], 0]])\n",
    "brow_ctrl_points.append([[lyric.phoneme_intervals[-1][1], 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Compute additional Eye Brow Movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brow_ctrl_pts(max_mean, mean_min, prev_freq, next_freq, mean_freq, time_range):\n",
    "    max_fluctuation = 1\n",
    "    rtv_raise = []\n",
    "    rtv_furrow = []\n",
    "    prev_freq_relative = prev_freq - mean_freq\n",
    "    next_freq_relative = next_freq - mean_freq\n",
    "    if prev_freq_relative < 0 and next_freq_relative < 0:\n",
    "        rtv_furrow.append([time_range[0], abs(max_fluctuation * prev_freq_relative/mean_min)])\n",
    "        rtv_furrow.append([time_range[1], abs(max_fluctuation * next_freq_relative/mean_min)])\n",
    "    elif prev_freq_relative > 0 and next_freq_relative > 0:\n",
    "        rtv_raise.append([time_range[0], max_fluctuation * prev_freq_relative/max_mean])\n",
    "        rtv_raise.append([time_range[1], max_fluctuation * next_freq_relative/max_mean])\n",
    "    elif prev_freq_relative >= 0 and next_freq_relative <= 0:\n",
    "        slope = (next_freq - prev_freq)/(time_range[1] - time_range[0])\n",
    "        time_to_zero_crossing = prev_freq_relative/(next_freq - prev_freq) * (time_range[1] - time_range[0])\n",
    "        rtv_raise.append([time_range[0], max_fluctuation * prev_freq_relative/max_mean])\n",
    "        rtv_raise.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_furrow.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_furrow.append([time_range[1], abs(max_fluctuation * next_freq_relative/mean_min)])\n",
    "    elif prev_freq_relative <= 0 and next_freq_relative <= 0:\n",
    "        slope = (next_freq - prev_freq)/(time_range[1] - time_range[0])\n",
    "        time_to_zero_crossing = prev_freq_relative/(next_freq - prev_freq) * (time_range[1] - time_range[0])\n",
    "        rtv_furrow.append([time_range[0], abs(max_fluctuation * prev_freq_relative/mean_min)])\n",
    "        rtv_furrow.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_raise.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_raise.append([time_range[1], max_fluctuation * next_freq_relative/max_mean])\n",
    "    return rtv_raise, rtv_furrow\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "finer_brow_raise_ctrl_points = [[0, 0]]\n",
    "finer_brow_furrow_ctrl_points = [[0, 0]]\n",
    "\n",
    "\n",
    "freq = lyric.pitch.selected_array[\"frequency\"]\n",
    "xs = lyric.xs\n",
    "freq[freq == 0] = np.nan\n",
    "mask = np.isnan(freq)\n",
    "freq[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), freq[~mask])\n",
    "freq = savgol_filter(freq, 61, 1)\n",
    "f = interp1d(xs, freq, kind=\"linear\")\n",
    "for sentence in sentences:\n",
    "    all_pitch_intervals_slope = []\n",
    "    all_pitch_intervals_time = []\n",
    "    si = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    fs = f(np.arange(si[0], min(si[1], xs[-1]), 0.01))\n",
    "    max_fs = fs.max()\n",
    "    min_fs = fs.min()\n",
    "    mean_fs = (max_fs * 0.7 + min_fs * 0.3)\n",
    "    # the starting point is always at \n",
    "    starting_freq = fs[0]\n",
    "                \n",
    "    for phone in sentence:\n",
    "        if len(lyric.pitch_slopes[phone]) == 0:\n",
    "            pitch_interval_time = lyric.phoneme_intervals[phone]\n",
    "            prev_freq = f(min(pitch_interval_time[0], xs[-1]))\n",
    "            next_freq = f(min(pitch_interval_time[1], xs[-1]))\n",
    "            raise_ctrl_pts_i, furrow_ctrl_pts_i = get_brow_ctrl_pts(max_fs-mean_fs, \n",
    "            mean_fs-min_fs, prev_freq, next_freq, mean_fs, pitch_interval_time)\n",
    "            finer_brow_raise_ctrl_points.extend(raise_ctrl_pts_i)\n",
    "            finer_brow_furrow_ctrl_points.extend(furrow_ctrl_pts_i)\n",
    "            \n",
    "        else:\n",
    "            for i in range(0, len(lyric.pitch_slopes[phone])):\n",
    "                pitch_interval_time = lyric.pitch_intervals[phone][i]\n",
    "                prev_freq = f(min(pitch_interval_time[0], xs[-1]))\n",
    "                next_freq = f(min(pitch_interval_time[1], xs[-1]))\n",
    "                raise_ctrl_pts_i, furrow_ctrl_pts_i = get_brow_ctrl_pts(max_fs-mean_fs, \n",
    "                mean_fs-min_fs, prev_freq, next_freq, mean_fs, pitch_interval_time)\n",
    "                finer_brow_raise_ctrl_points.extend(raise_ctrl_pts_i)\n",
    "                finer_brow_furrow_ctrl_points.extend(furrow_ctrl_pts_i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'viseme_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_1140/221643519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m output ={\"viseme\":[viseme_list, viseme_intervals],\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;34m\"brow\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrow_movement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrow_ctrl_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiner_brow_raise_ctrl_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiner_brow_furrow_ctrl_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;34m\"blink\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meye_movement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meye_ctrl_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m\"jaw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaw_ctrl_pts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;34m\"lip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlip_ctrl_pts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'viseme_list' is not defined"
     ]
    }
   ],
   "source": [
    "output ={\"viseme\":[viseme_list, viseme_intervals],\n",
    "        \"brow\":[brow_movement, brow_ctrl_points, finer_brow_raise_ctrl_points, finer_brow_furrow_ctrl_points],\n",
    "        \"blink\":[eye_movement, eye_ctrl_points],\n",
    "        \"jaw\":jaw_ctrl_pts,\n",
    "        \"lip\":lip_ctrl_pts, \n",
    "        \"vib\":vib_ctrl_pts}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootleg Jali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.CMU2JALI import *\n",
    "CMU_VOCABULARY = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G',\n",
    "                  'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "                  'UW', 'V', 'W', 'Y', 'Z', 'ZH'])\n",
    "LIP_HEAVY_VISEMES_JALI = set([\"Oh_pointer\", \"W_pointer\", \"U_pointer\", \"SZ_pointer\", \"JY_pointer\"])\n",
    "NASAL_OBSTRUENTS_JALI = set([\"LNTD_pointer\", \"GK_pointer\", \"FV_pointer\", \"MBP_pointer\", ])\n",
    "LABIAL_AND_DENTAL_JALI = set([\"MBP_pointer\", \"SZ_pointer\", \"FV_pointer\"])\n",
    "SEMIVOWELS_CMU = set([\"Y\", \"W\", \"H\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lyric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f943f267ee7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphoneme_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mphoneme_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lyric' is not defined"
     ]
    }
   ],
   "source": [
    "phoneme_list = lyric.phoneme_list\n",
    "phoneme_interval = lyric.phoneme_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kth_neighbour(input_list, i, k):\n",
    "    if i+k < 0 or i+k >= len(input_list):\n",
    "        return None\n",
    "    return input_list[i+k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "viseme_list = []\n",
    "viseme_intervals = []\n",
    "phoneme_list_pure = []\n",
    "prev_vowel = \"Uh\"\n",
    "next_vowel = \"\"\n",
    "# pass 1\n",
    "for i in range(0, len(phoneme_list)):\n",
    "    if phoneme_list[i] in CMU_VOCABULARY:\n",
    "        onset = 0.12\n",
    "        offset = 0.12\n",
    "        viseme_jali = CMU2VISEME[phoneme_list[i]]+\"_pointer\"\n",
    "        if viseme_jali in LIP_HEAVY_VISEMES_JALI:\n",
    "            onset = 0.16\n",
    "            offset = 0.16\n",
    "        start = phoneme_interval[i][0]\n",
    "        end = phoneme_interval[i][1]\n",
    "        if (end - start) <= 0.4:\n",
    "            value = 5\n",
    "            sustain = 0.75\n",
    "            decay = 0.75\n",
    "        else:\n",
    "            value = 8\n",
    "            sustain = 0.9\n",
    "            decay = 0.95\n",
    "        \n",
    "        if not viseme_jali in VOWELS_JALI:\n",
    "            value = 9\n",
    "        viseme_curve = generate_animation_ctrl_pts(start, end, value, sustain=0.75, decay=decay, onset=onset, offset=offset)\n",
    "        viseme_list.append(viseme_jali)\n",
    "        phoneme_list_pure.append(phoneme_list[i])\n",
    "        viseme_intervals.append(viseme_curve)\n",
    "# pass 2 enforcing co-articulation\n",
    "viseme_list_final = []\n",
    "viseme_intervals_final = []\n",
    "\n",
    "i = 0;\n",
    "prev_vowel = 0\n",
    "prev_consonants = 0\n",
    "\n",
    "while i < len(viseme_list):\n",
    "    increment = 1\n",
    "    i_next = min(i + 1, len(viseme_list)-1)\n",
    "    if (viseme_list[i_next] == viseme_list[i] and\n",
    "       viseme_list[i_next] in CONSONANTS_JALI):\n",
    "        viseme_list_final.append(viseme_list[i_next])\n",
    "        int_curr = viseme_intervals[i]\n",
    "        int_next = viseme_intervals[i_next]\n",
    "        viseme_interval = [int_curr[0], int_curr[1], int_next[0], int_next[1]]\n",
    "        increment = 2\n",
    "    elif phoneme_list_pure[i] in SEMIVOWELS_CMU and viseme_list[i_next] in VOWELS_JALI:\n",
    "        viseme_list_final.append(viseme_list[i_next])\n",
    "        int_curr = viseme_intervals[i]\n",
    "        int_next = viseme_intervals[i_next]\n",
    "        viseme_interval = [int_curr[0], int_curr[1], int_next[0], int_next[1]]\n",
    "        increment = 2\n",
    "    elif viseme_list[i] in LIP_HEAVY_VISEMES_JALI:\n",
    "        current_interval = viseme_intervals[i] \n",
    "        if not get_kth_neighbour(viseme_list, i, -1) is None:\n",
    "            current_interval[0][0] = viseme_intervals[i-1][0][0]\n",
    "            current_interval[1][0] = viseme_intervals[i-1][1][0]\n",
    "        if not get_kth_neighbour(viseme_list, i, +1) is None:\n",
    "            current_interval[2][0] = viseme_intervals[i+1][0][0]\n",
    "            current_interval[3][0] = viseme_intervals[i+1][1][0]\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(current_interval)\n",
    "    elif viseme_list[i] in NASAL_OBSTRUENTS_JALI:\n",
    "        # these have no effect on Jaw if they are not Sibilants\n",
    "        temp_interval = viseme_intervals[i]\n",
    "        length = temp_interval[2][0]-temp_interval[1][0]\n",
    "        if length > 1/25:\n",
    "            viseme_list_final.append(viseme_list[i])\n",
    "            viseme_intervals_final.append(viseme_intervals[i])\n",
    "        else:    \n",
    "            temp = viseme_list[i].split(\"_\")\n",
    "            viseme_list_final.append(temp[0] + \"a_\" + temp[1])\n",
    "            viseme_intervals_final.append(viseme_intervals[i])\n",
    "    else:\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(viseme_intervals[i])\n",
    "        \n",
    "    if viseme_list[i] in CONSONANTS_JALI:\n",
    "        current_consonant_id = i\n",
    "    elif viseme_list[i] in VOWELS_JALI:\n",
    "        current_vowel_id = i\n",
    "    i = i + increment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a: Jali parameter version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jali_parameters\n",
    "DEFALT_JALI_VAL = 6\n",
    "\n",
    "jaw_ctrl_pts = []\n",
    "lip_ctrl_pts = []\n",
    "\n",
    "# get information on the frequency\n",
    "freq = lyric.pitch.selected_array[\"frequency\"]\n",
    "xs = lyric.xs\n",
    "freq[freq == 0] = np.nan\n",
    "mask = np.isnan(freq)\n",
    "freq[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), freq[~mask])\n",
    "freq = savgol_filter(freq, 21, 3)\n",
    "f = interp1d(xs, freq, kind=\"linear\")\n",
    "for sentence in sentences:\n",
    "    all_pitch_intervals_slope = []\n",
    "    all_pitch_intervals_time = []\n",
    "    si = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    fs = f(np.arange(si[0], min(si[1], xs[-1]), 0.01))\n",
    "    max_fs = fs.max()\n",
    "    min_fs = fs.min()\n",
    "    # the starting point is always at \n",
    "    current_freq = fs[0]\n",
    "    jaw_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, 6 + (current_freq-min_fs)/(max_fs-min_fs) * 4]\n",
    "    jaw_ctrl_pts.append(jaw_ctrl_pt_0)\n",
    "    lip_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, 6 + (current_freq-min_fs)/(max_fs-min_fs) * 4]\n",
    "    lip_ctrl_pts.append(lip_ctrl_pt_0)\n",
    "    prev_voice_type = \"chest\"\n",
    "    for phone in sentence:\n",
    "        if len(lyric.pitch_slopes[phone]) == 0 and len(lyric.voice_quality_lists[phone]) > 0:\n",
    "            pitch_interval_time = lyric.phoneme_intervals[phone]\n",
    "            if lyric.voice_quality_lists[phone][0] == \"head\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = f(pitch_interval_time[0])\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[1])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"head\"\n",
    "            elif lyric.voice_quality_lists[phone][0] == \"belt\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0])\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], new_lip_val])\n",
    "                if prev_voice_type == \"head\":\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"belt\"\n",
    "        for i in range(0, len(lyric.pitch_slopes[phone])):\n",
    "            pitch_interval_time = lyric.pitch_intervals[phone][i]\n",
    "            if lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"head\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"head\"\n",
    "            elif lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"belt\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], new_lip_val])\n",
    "                if prev_voice_type == \"head\":\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"belt\"\n",
    "            elif lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"chest\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "#                 if prev_voice_type == \"head\":\n",
    "#                     jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "#                     jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"chest\"\n",
    "            elif lyric.pitch_slopes[phone][i] < 0: # these are opportunities to bring \n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], max(new_jaw_val, DEFALT_JALI_VAL)])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], max(new_lip_val, DEFALT_JALI_VAL)])\n",
    "                prev_voice_type = lyric.voice_quality_intervals[phone][i]\n",
    "            elif ((prev_voice_type == \"head\" and lyric.voice_quality_intervals[phone][i] == \"belt\") or \n",
    "                  (prev_voice_type == \"belt\" and lyric.voice_quality_intervals[phone][i] == \"head\")):\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], prev_jaw_val])\n",
    "                prev_voice_type = lyric.voice_quality_intervals[phone][i]\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_ctrl_pts = []\n",
    "for k in lyric.vibrato_intervals:\n",
    "    if len(k) > 0:\n",
    "        for m in k:\n",
    "            vib_ctrl_pts.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\"viseme\":[viseme_list, viseme_intervals],\n",
    "        \"brow\":[brow_movement, brow_ctrl_points, finer_brow_raise_ctrl_points, finer_brow_furrow_ctrl_points],\n",
    "        \"blink\":[eye_movement, eye_ctrl_points],\n",
    "        \"jaw\":jaw_ctrl_pts,\n",
    "        \"lip\":lip_ctrl_pts, \n",
    "        \"vib\":vib_ctrl_pts}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple head movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_shake(t_start, t_end, prev_motion_x, prev_motions_y):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "\n",
    "x_dir_head = []\n",
    "y_dir_head = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying eye brow movements using facial landmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.facial_landmarking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'E:/facial_data_analysis_videos/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_2640/1733076059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvideo_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"video.mp4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"E:/facial_data_analysis_videos/1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m extract_landmarks_media_pipe(video_title[0],\n\u001b[0m\u001b[0;32m      4\u001b[0m                          video_path[0], save_annotated_video=True)\n",
      "\u001b[1;32m~\\Desktop\\jali_sing\\util\\facial_landmarking.py\u001b[0m in \u001b[0;36mextract_landmarks_media_pipe\u001b[1;34m(input_video, input_dir, show_annotated_video, show_normalized_pts, save_annotated_video, tolerance)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#     return output_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mget_audio_from_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_video\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# set up cv2 object for querying images from video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\jali_sing\\util\\ioUtil.py\u001b[0m in \u001b[0;36mget_audio_from_video\u001b[1;34m(file_name, video_folder_path, target_fps, remove)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_audio_from_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdir_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_folder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The directory is empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'E:/facial_data_analysis_videos/1'"
     ]
    }
   ],
   "source": [
    "video_title = [\"video.mp4\"]\n",
    "video_path = [\"E:/facial_data_analysis_videos/1\"]\n",
    "extract_landmarks_media_pipe(video_title[0],\n",
    "                         video_path[0], save_annotated_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jali_sing",
   "language": "python",
   "name": "jali_sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
