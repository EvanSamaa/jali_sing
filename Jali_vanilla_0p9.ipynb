{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.sound_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from util.sound_processing import *\n",
    "VOWELS = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW', \"ER\", \"N\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Animation Generation.ipynb
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e9cfeb394a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"E:/Structured_data/rolling_in_the_deep_adele\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_lyric_alignment_textgrids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/jali_sing/util/sound_processing.py\u001b[0m in \u001b[0;36mcombine_lyric_alignment_textgrids\u001b[0;34m(dir, file_name_template)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;31m# get all the textgrid_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0msub_audio_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_audio_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextgrid_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\S*TextGrid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mtextgrid_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextgrid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Structured_data/rolling_in_the_deep_adele/temp'"
     ]
    }
   ],
   "source": [
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep2_adale\"\n",
    "file_name_template = \"audio_vocals\"\n",
    "lyric = PraatScript_Lyric_Wrapper(os.path.join(dir, file_name_template+\".wav\"), os.path.join(dir, file_name_template+\".txt\"), \n",
    "                                  sentence_textgrids_path=[dir+\"/audio_vocals_fixed.TextGrid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> 229a94fc14954f353149f1ac4f1aebc6640532e9:Jali_vanilla_0p9.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep_adele\"\n",
    "file_name_template = \"audio\"\n",
    "lyric = combine_lyric_alignment_textgrids(dir, file_name_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric.compute_self_pitch_intervals()\n",
    "lyric.compute_self_vibrato_intervals()\n",
    "lyric.compute_self_singing_style_intervals()\n",
    "lyric.write_textgrid(dir, file_name_template+\"_new\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Animation Generation.ipynb
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> 229a94fc14954f353149f1ac4f1aebc6640532e9:Jali_vanilla_0p9.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation_ctrl_pts(start, end, value, sustain=1, decay = 0.75, onset=0.1, offset=0):\n",
    "    interval = []\n",
    "    interval.append([start-onset, 0])\n",
    "    # second point is when the belting starts \n",
    "    interval.append([start, 1 * value])\n",
    "    # third point emphasizes decay, it happens 75% down the interval\n",
    "    if sustain < 1:\n",
    "        interval.append([start + sustain * (end - start), decay * value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    elif sustain == 1:\n",
    "        interval.append([end, value])\n",
    "        # last point is where the furrowing ends\n",
    "        interval.append([end+offset, 0])\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Eye Brow Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Compute Eye Brow Movements\n",
    "This will change the data format for saving, and use more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "# sentence stores the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is mostly for word level i.e. long vowels in a word. It would also be necessary to explore\n",
    "# sentence level expressions (possibly from studying another song)\n",
    "# this will have array of either 3 or 4 control points. \n",
    "# i.e. [[[ctrl_pt_1]...[ctrl_pt_k]], [[ctrl_pt_1]...[ctrl_pt_k]]] \n",
    "brow_movement = []\n",
    "brow_ctrl_points = []\n",
    "eye_movement = []\n",
    "eye_ctrl_points = []\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    has_belt_pitch_interval_id = -1\n",
    "    has_belt_word_id = -1\n",
    "    has_head_pitch_interval_id = -1\n",
    "    has_head_word_id = -1\n",
    "    only_has_pitch_interval_id = -1\n",
    "    only_has_head_word_id = -1\n",
    "    \n",
    "    for phone_id in sentence:\n",
    "        phone = lyric.phoneme_list[phone_id]\n",
    "        voice_qualities = lyric.coarse_voice_quality_lists[phone_id]\n",
    "        voice_intervals = lyric.coarse_voice_quality_intervals[phone_id]\n",
    "        # look to see if there are any parts that has belting\n",
    "        for voice_quality_id in range(0, len(voice_qualities)):\n",
    "            if (voice_qualities[voice_quality_id] == \"belt\" and has_belt_word_id < 0 and \n",
    "                voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.4):\n",
    "                has_belt_word_id = phone_id\n",
    "                has_belt_pitch_interval_id = voice_quality_id\n",
    "            elif (voice_qualities[voice_quality_id] == \"head\" and has_belt_word_id >= 0 and has_head_word_id < 0 and \n",
    "                 voice_intervals[voice_quality_id][1] -  voice_intervals[voice_quality_id][0] >= 0.2):\n",
    "                has_head_word_id = phone_id\n",
    "                has_head_pitch_interval_id = voice_quality_id\n",
    "            if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "                break\n",
    "                \n",
    "    # do a second pass looking for brow raises\n",
    "    if has_belt_word_id < 0 and has_head_word_id < 0:\n",
    "        for phone_id in sentence:\n",
    "            phone = lyric.phoneme_list[phone_id]\n",
    "            pitch_change_interval = lyric.pitch_intervals[phone_id]\n",
    "            pitch_change_slopes = lyric.pitch_slopes[phone_id]\n",
    "            # look to see if there are any parts that has belting\n",
    "            for vi in range(0, len(pitch_change_slopes)):\n",
    "                if pitch_change_slopes[vi] >= 100 and only_has_head_word_id < 0:\n",
    "                    only_has_head_word_id = phone_id \n",
    "                    only_has_pitch_interval_id = vi\n",
    "                    break\n",
    "            if only_has_head_word_id >= 0:\n",
    "                break\n",
    "                \n",
    "    # this section of the code deal with having head voice only segments and the eye\n",
    "    # brow raising in those \n",
    "    if only_has_head_word_id > 0:\n",
    "        value = 5\n",
    "        onset = lyric.pitch_intervals[only_has_head_word_id][0][0]\n",
    "        start = lyric.pitch_intervals[only_has_head_word_id][only_has_pitch_interval_id][0] # where belting starts\n",
    "        onset = start - onset\n",
    "        end = lyric.pitch_intervals[only_has_head_word_id][-1][1]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, onset)\n",
    "        brow_movement.append(\"raise\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "    # this section of the code deal with belting and the related physiological points of the eyes\n",
    "    if has_belt_word_id > 0 and has_head_word_id > 0:\n",
    "        # deal with the furrowing related movements\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "                                                                                             # end of belting   \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        # deal with the eyebrow raise related movements\n",
    "         \n",
    "        value = 5\n",
    "        start = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0]\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][1]        \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.1)\n",
    "        brow_ctrl_points.append(interval)\n",
    "        brow_movement.append(\"raise\")\n",
    "        \n",
    "        # deal with eye openning and closing\n",
    "        value = 10\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0] # where belting starts\n",
    "        end = lyric.voice_quality_intervals[has_head_word_id][has_head_pitch_interval_id][0] # where head voice starts i.e. end \n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "        \n",
    "    elif has_belt_word_id > 0 and has_head_word_id < 0:\n",
    "        # if there is belting, but sentence do not end with the use of head voice\n",
    "        value = 8\n",
    "        start = lyric.voice_quality_intervals[has_belt_word_id][has_belt_pitch_interval_id][0]\n",
    "        start = min(start - 0.1, lyric.voice_quality_intervals[has_belt_word_id][0][0]) \n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 0.75, 0.05)\n",
    "        brow_movement.append(\"furrow\")\n",
    "        brow_ctrl_points.append(interval)\n",
    "        \n",
    "        value = 10\n",
    "        end = lyric.phoneme_intervals[sentence[-1]][1]-0.1 # end of this sentence if there is no next sentence\n",
    "        if i < len(sentences)-1:\n",
    "            # the end will be the first word from the next sentence if there are more than one sentence\n",
    "            start_of_next_sentence = -1\n",
    "            next_sentence = sentences[i+1]\n",
    "            for phone_id in next_sentence:\n",
    "                if lyric.phoneme_list[phone_id] in VOWELS:\n",
    "                    start_of_next_sentence = phone_id\n",
    "                    break\n",
    "            end = lyric.phoneme_intervals[start_of_next_sentence][0]\n",
    "        interval = generate_animation_ctrl_pts(start, end, value, 1, 1, 0.05, 0.1)\n",
    "        eye_movement.append(\"closure\")\n",
    "        eye_ctrl_points.append(interval)\n",
    "brow_movement.append(\"raise\")\n",
    "brow_movement.append(\"furrow\")\n",
    "brow_ctrl_points.append([[lyric.phoneme_intervals[-1][1], 0]])\n",
    "brow_ctrl_points.append([[lyric.phoneme_intervals[-1][1], 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Compute additional Eye Brow Movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brow_ctrl_pts(max_mean, mean_min, prev_freq, next_freq, mean_freq, time_range):\n",
    "    max_fluctuation = 1\n",
    "    rtv_raise = []\n",
    "    rtv_furrow = []\n",
    "    prev_freq_relative = prev_freq - mean_freq\n",
    "    next_freq_relative = next_freq - mean_freq\n",
    "    if prev_freq_relative < 0 and next_freq_relative < 0:\n",
    "        rtv_furrow.append([time_range[0], abs(max_fluctuation * prev_freq_relative/mean_min)])\n",
    "        rtv_furrow.append([time_range[1], abs(max_fluctuation * next_freq_relative/mean_min)])\n",
    "    elif prev_freq_relative > 0 and next_freq_relative > 0:\n",
    "        rtv_raise.append([time_range[0], max_fluctuation * prev_freq_relative/max_mean])\n",
    "        rtv_raise.append([time_range[1], max_fluctuation * next_freq_relative/max_mean])\n",
    "    elif prev_freq_relative >= 0 and next_freq_relative <= 0:\n",
    "        slope = (next_freq - prev_freq)/(time_range[1] - time_range[0])\n",
    "        time_to_zero_crossing = prev_freq_relative/(next_freq - prev_freq) * (time_range[1] - time_range[0])\n",
    "        rtv_raise.append([time_range[0], max_fluctuation * prev_freq_relative/max_mean])\n",
    "        rtv_raise.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_furrow.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_furrow.append([time_range[1], abs(max_fluctuation * next_freq_relative/mean_min)])\n",
    "    elif prev_freq_relative <= 0 and next_freq_relative <= 0:\n",
    "        slope = (next_freq - prev_freq)/(time_range[1] - time_range[0])\n",
    "        time_to_zero_crossing = prev_freq_relative/(next_freq - prev_freq) * (time_range[1] - time_range[0])\n",
    "        rtv_furrow.append([time_range[0], abs(max_fluctuation * prev_freq_relative/mean_min)])\n",
    "        rtv_furrow.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_raise.append([time_range[0] + time_to_zero_crossing, 0])\n",
    "        rtv_raise.append([time_range[1], max_fluctuation * next_freq_relative/max_mean])\n",
    "    return rtv_raise, rtv_furrow\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finer_brow_raise_ctrl_points = [[0, 0]]\n",
    "finer_brow_furrow_ctrl_points = [[0, 0]]\n",
    "\n",
    "\n",
    "freq = lyric.pitch.selected_array[\"frequency\"]\n",
    "xs = lyric.xs\n",
    "freq[freq == 0] = np.nan\n",
    "mask = np.isnan(freq)\n",
    "freq[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), freq[~mask])\n",
    "freq = savgol_filter(freq, 61, 1)\n",
    "f = interp1d(xs, freq, kind=\"linear\")\n",
    "for sentence in sentences:\n",
    "    all_pitch_intervals_slope = []\n",
    "    all_pitch_intervals_time = []\n",
    "    si = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    fs = f(np.arange(si[0], min(si[1], xs[-1]), 0.01))\n",
    "    max_fs = fs.max()\n",
    "    min_fs = fs.min()\n",
    "    mean_fs = (max_fs * 0.7 + min_fs * 0.3)\n",
    "    # the starting point is always at \n",
    "    starting_freq = fs[0]\n",
    "                \n",
    "    for phone in sentence:\n",
    "        if len(lyric.pitch_slopes[phone]) == 0:\n",
    "            pitch_interval_time = lyric.phoneme_intervals[phone]\n",
    "            prev_freq = f(min(pitch_interval_time[0], xs[-1]))\n",
    "            next_freq = f(min(pitch_interval_time[1], xs[-1]))\n",
    "            raise_ctrl_pts_i, furrow_ctrl_pts_i = get_brow_ctrl_pts(max_fs-mean_fs, \n",
    "            mean_fs-min_fs, prev_freq, next_freq, mean_fs, pitch_interval_time)\n",
    "            finer_brow_raise_ctrl_points.extend(raise_ctrl_pts_i)\n",
    "            finer_brow_furrow_ctrl_points.extend(furrow_ctrl_pts_i)\n",
    "            \n",
    "        else:\n",
    "            for i in range(0, len(lyric.pitch_slopes[phone])):\n",
    "                pitch_interval_time = lyric.pitch_intervals[phone][i]\n",
    "                prev_freq = f(min(pitch_interval_time[0], xs[-1]))\n",
    "                next_freq = f(min(pitch_interval_time[1], xs[-1]))\n",
    "                raise_ctrl_pts_i, furrow_ctrl_pts_i = get_brow_ctrl_pts(max_fs-mean_fs, \n",
    "                mean_fs-min_fs, prev_freq, next_freq, mean_fs, pitch_interval_time)\n",
    "                finer_brow_raise_ctrl_points.extend(raise_ctrl_pts_i)\n",
    "                finer_brow_furrow_ctrl_points.extend(furrow_ctrl_pts_i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootleg Jali"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Animation Generation.ipynb
   "execution_count": 4,
=======
   "execution_count": 41,
>>>>>>> 229a94fc14954f353149f1ac4f1aebc6640532e9:Jali_vanilla_0p9.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.CMU2JALI import *\n",
    "CMU_VOCABULARY = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G',\n",
    "                  'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "                  'UW', 'V', 'W', 'Y', 'Z', 'ZH'])\n",
    "LIP_HEAVY_VISEMES_JALI = set([\"Oh_pointer\", \"W_pointer\", \"U_pointer\", \"SZ_pointer\", \"JY_pointer\"])\n",
    "NASAL_OBSTRUENTS_JALI = set([\"LNTD_pointer\", \"GK_pointer\", \"FV_pointer\", \"MBP_pointer\", ])\n",
    "LABIAL_AND_DENTAL_JALI = set([\"MBP_pointer\", \"SZ_pointer\", \"FV_pointer\", \"W_pointer\"])\n",
    "LABIAL_AND_DENTAL = set([\"M\", \"BP\", \"FV\"])\n",
    "LABIAL_AND_DENTAL_NO_JAW_JALI = set([\"MBPa_pointer\", \"SZa_pointer\", \"FVa_pointer\"])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Animation Generation.ipynb
   "execution_count": 5,
=======
   "execution_count": 42,
>>>>>>> 229a94fc14954f353149f1ac4f1aebc6640532e9:Jali_vanilla_0p9.ipynb
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lyric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f943f267ee7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphoneme_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mphoneme_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneme_intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lyric' is not defined"
     ]
    }
   ],
   "source": [
    "phoneme_list = lyric.phoneme_list\n",
    "phoneme_interval = lyric.phoneme_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kth_neighbour(input_list, i, k):\n",
    "    if i+k < 0 or i+k >= len(input_list):\n",
    "        return None\n",
    "    return input_list[i+k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.b Bootleg Jali Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU2VISEME = {\"AA\":\"Ah\",\n",
    "                    \"AO\":\"Ah\",\n",
    "                    \"AY\":\"Ah\",\n",
    "                    \"AW\":\"Ah\",\n",
    "\n",
    "                    \"AE\":\"Aa\",\n",
    "                    \"EY\":\"Ah\",\n",
    "\n",
    "                    \"UH\":\"Uh\",\n",
    "\n",
    "                    \"UW\":\"U\",\n",
    "\n",
    "                    \"IH\": \"Ih\",\n",
    "                    \"IY\": \"Ih\",\n",
    "\n",
    "                    \"EH\": \"Eh\",\n",
    "                    \"HH\": \"Eh\",\n",
    "                    \"UH\": \"Eh\",\n",
    "                    \"AH\": \"Eh\",\n",
    "                    \"ER\": \"Eh\",\n",
    "\n",
    "                    \"OW\":\"Oh\",\n",
    "                    \"OY\":\"Oh\",\n",
    "\n",
    "                    \"R\":\"R\",\n",
    "\n",
    "                    \"D\":\"LNTD\",\n",
    "                    \"T\": \"LNTD\",\n",
    "                    \"L\":\"LNTD\",\n",
    "                    \"N\":\"LNTD\",\n",
    "                    \"NG\":\"LNTD\",\n",
    "\n",
    "                    \"F\":\"FV\",\n",
    "                    \"V\":\"FV\",\n",
    "\n",
    "                    \"B\":\"BP\",\n",
    "                    \"M\":\"M\",\n",
    "                    \"P\":\"BP\",\n",
    "\n",
    "                    \"CH\":\"ShChZh\",\n",
    "                    \"SH\":\"ShChZh\",\n",
    "                    \"ZH\":\"ShChZh\",\n",
    "\n",
    "                    \"S\": \"SZ\",\n",
    "                    \"Z\": \"SZ\",\n",
    "\n",
    "                    \"DH\":\"Th\",\n",
    "                    \"TH\":\"Th\",\n",
    "\n",
    "                    \"G\":\"GK\",\n",
    "                    \"K\":\"GK\",\n",
    "\n",
    "                    \"Y\":\"Y\",\n",
    "                    \"JH\":\"J\",\n",
    "\n",
    "                    \"W\":\"W\",\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU_VOCABULARY = set(['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G',\n",
    "                  'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "                  'UW', 'V', 'W', 'Y', 'Z', 'ZH'])\n",
    "LIP_CLOSERS_CMU = set([\"B\", \"F\", \"M\", \"P\", \"S\", \"V\"])\n",
    "VOWELS_JALI = set(['Ih', 'Ee', 'Eh', 'Aa', 'U', 'Uh', 'Oo', 'Oh', 'Schwa', 'Eu', \"Ah\"])\n",
    "VOWELS_SLIDERS_JALI = set(['Ih_pointer', 'Ee_pointer', 'Eh_pointer', 'Aa_pointer', 'U_pointer', 'Uh_pointer'\n",
    "                           , 'Oo_pointer', 'Oh_pointer', 'Schwa_pointer', 'Eu_pointer', \"Ah_pointer\"])\n",
    "CONSONANTS_JALI = set([\"M_pointer\", \"BP_pointer\", \"JY_pointer\", \"Th_pointer\", \"ShChZh_pointer\", \"SZ_pointer\", \"GK_pointer\", \"LNTD_pointer\", \"R_pointer\", \"W_pointer\", \"FV_pointer\"])\n",
    "CONSONANTS_NOJAW_JALI = set([\"Ya_pointer\", \"Ja_pointer\", \"Ra_pointer\", \"FVa_pointer\", \"LNTDa_pointer\", \"Ma_pointer\", \"BPa_pointer\", \"Wa_pointer\", \"Tha_pointer\", \"GKa_pointer\"])\n",
    "JALI_SLIDERS_SET = set.union(VOWELS_SLIDERS_JALI, CONSONANTS_JALI, CONSONANTS_NOJAW_JALI)\n",
    "ALL_CONSONANTS_JALI = set.union(CONSONANTS_NOJAW_JALI, CONSONANTS_JALI)\n",
    "SIBLANT_CONSONANTS_JALI = set([\"SZ_pointer\", \"ShChZh_pointer\"])\n",
    "SIBLANT_JALI = set([\"SZ\", \"ShChZh\"])\n",
    "NASAL_OBSTRUENTS_JALI = set([\"LNTD\", \"GK\", \"FV\", \"M\", \"BP\"])\n",
    "LIP_ROUNDER_CONSONANT_JALI = set([\"M_pointer\", \"BP_pointer\", \"FV_pointer\"])\n",
    "LIP_ROUNDER_CONSONANT_JALI_DICT = {\"M_pointer\":\"Ma_pointer\", \"BP_pointer\":\"BPa_pointer\", \"FV_pointer\":\"FVa_pointer\"}\n",
    "LIP_HEAVY_VISEMES_JALI = set([\"Oh_pointer\", \"W_pointer\", \"Wa_pointer\", \"U_pointer\", \"SZ_pointer\", \"JY_pointer\",\n",
    "                             \"Ya_pointer\", \"Ja_pointer\"])\n",
    "SEMIVOWELS_CMU = set([\"HH\"])\n",
    "prev_slider_dict = {}\n",
    "for i in range(0, len(list(JALI_SLIDERS_SET))):\n",
    "    prev_slider_dict[list(JALI_SLIDERS_SET)[i]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_list = lyric.phoneme_list\n",
    "phoneme_interval = lyric.phoneme_intervals\n",
    "def get_kth_neighbour(input_list, i, k):\n",
    "    if i+k < 0 or i+k >= len(input_list):\n",
    "        return None\n",
    "    return input_list[i+k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viseme_list = []\n",
    "viseme_intervals = []\n",
    "phoneme_list_pure = []\n",
    "prev_vowel = \"Uh\"\n",
    "next_vowel = \"\"\n",
    "# pass 1\n",
    "for i in range(0, len(phoneme_list)):\n",
    "    if phoneme_list[i] in CMU_VOCABULARY:\n",
    "        onset = 0.12\n",
    "        offset = 0.12\n",
    "        if CMU2VISEME[phoneme_list[i]] in VOWELS_JALI or CMU2VISEME[phoneme_list[i]] in SIBLANT_JALI:\n",
    "            viseme_jali = CMU2VISEME[phoneme_list[i]] + \"_pointer\"\n",
    "        else :\n",
    "            if CMU2VISEME[phoneme_list[i]] in NASAL_OBSTRUENTS_JALI and phoneme_interval[i][1] - phoneme_interval[i][0] > 1/20:\n",
    "                viseme_jali = CMU2VISEME[phoneme_list[i]] + \"_pointer\"\n",
    "            else:\n",
    "                viseme_jali = CMU2VISEME[phoneme_list[i]] + \"a_pointer\"\n",
    "        if viseme_jali in LIP_HEAVY_VISEMES_JALI:\n",
    "            onset = 0.16\n",
    "            offset = 0.16\n",
    "        start = phoneme_interval[i][0]\n",
    "        end = phoneme_interval[i][1]\n",
    "        if (end - start) <= 0.1:\n",
    "            value = 6\n",
    "            sustain = 0.75\n",
    "            decay = 0.75\n",
    "        elif (end - start) <= 0.3:\n",
    "            value = 6\n",
    "            sustain = 0.75\n",
    "            decay = 0.75\n",
    "        else:\n",
    "            value = 8\n",
    "            sustain = 0.75\n",
    "            decay = 0.75\n",
    "        if phoneme_list[i] in LIP_CLOSERS_CMU:\n",
    "            value = 10\n",
    "        viseme_curve = generate_animation_ctrl_pts(start, end, value, sustain=sustain, decay=decay, onset=onset, offset=offset)\n",
    "        viseme_list.append(viseme_jali)\n",
    "        phoneme_list_pure.append(phoneme_list[i])\n",
    "        viseme_intervals.append(viseme_curve)\n",
    "# pass 2 enforcing co-articulation\n",
    "viseme_list_final = []\n",
    "viseme_intervals_final = []\n",
    "i = 0;\n",
    "\n",
    "while i < len(viseme_list):\n",
    "    increment = 1\n",
    "    i_next = min(i + 1, len(viseme_list)-1)\n",
    "    if (viseme_list[i_next] == viseme_list[i]):\n",
    "        viseme_list_final.append(viseme_list[i_next])\n",
    "        int_curr = viseme_intervals[i]\n",
    "        int_next = viseme_intervals[i_next]\n",
    "        viseme_interval = [int_curr[0], [int_curr[1][0], max(int_curr[1][1], int_next[1][1])], \n",
    "                           [int_next[2][0], max(int_curr[2][1], int_next[2][1])], int_next[3]]\n",
    "        viseme_intervals_final.append(viseme_interval)\n",
    "        if viseme_list[i_next] in LIP_ROUNDER_CONSONANT_JALI:\n",
    "            viseme_list_final.append(LIP_ROUNDER_CONSONANT_JALI_DICT[viseme_list[i_next]])\n",
    "            viseme_intervals_final.append(viseme_interval)\n",
    "        increment = 2\n",
    "    elif phoneme_list_pure[i] in SEMIVOWELS_CMU and viseme_list[i_next].split(\"_\")[0] in VOWELS_JALI:\n",
    "        viseme_list_final.append(viseme_list[i_next])\n",
    "        int_curr = viseme_intervals[i]\n",
    "        int_next = viseme_intervals[i_next]\n",
    "        viseme_interval = [int_curr[0], [int_curr[1][0], max(int_curr[1][1], int_next[1][1])], \n",
    "                   [int_next[2][0], max(int_curr[2][1], int_next[2][1])], int_next[3]]\n",
    "        viseme_intervals_final.append(viseme_interval)\n",
    "        if viseme_list[i_next] in LIP_ROUNDER_CONSONANT_JALI:\n",
    "            viseme_list_final.append(LIP_ROUNDER_CONSONANT_JALI_DICT[viseme_list[i_next]])\n",
    "            viseme_intervals_final.append(viseme_interval)\n",
    "        increment = 2\n",
    "    elif viseme_list[i] in LIP_HEAVY_VISEMES_JALI:\n",
    "        current_interval = viseme_intervals[i] \n",
    "        if not get_kth_neighbour(viseme_list, i, -1) is None:\n",
    "            current_interval[0][0] = viseme_intervals[i-1][0][0]\n",
    "            current_interval[1][0] = viseme_intervals[i-1][1][0]\n",
    "        if not get_kth_neighbour(viseme_list, i, +1) is None:\n",
    "            current_interval[2][0] = viseme_intervals[i+1][0][0]\n",
    "            current_interval[3][0] = viseme_intervals[i+1][1][0]\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(current_interval)\n",
    "        if viseme_list[i] in LIP_ROUNDER_CONSONANT_JALI:\n",
    "            viseme_list_final.append(LIP_ROUNDER_CONSONANT_JALI_DICT[viseme_list[i]])\n",
    "            viseme_intervals_final.append(current_interval)\n",
    "    else:\n",
    "        viseme_list_final.append(viseme_list[i])\n",
    "        viseme_intervals_final.append(viseme_intervals[i])\n",
    "        if viseme_list[i] in LIP_ROUNDER_CONSONANT_JALI:\n",
    "            viseme_list_final.append(LIP_ROUNDER_CONSONANT_JALI_DICT[viseme_list[i]])\n",
    "            viseme_intervals_final.append(viseme_intervals[i])\n",
    "    i = i + increment\n",
    "# pass 3\n",
    "# set this up\n",
    "for i in range(0, len(list(JALI_SLIDERS_SET))):\n",
    "    prev_slider_dict[list(JALI_SLIDERS_SET)[i]] = -1\n",
    "viseme_list_final_final = []\n",
    "viseme_intervals_final_final = []\n",
    "i = 0  \n",
    "while i < len(viseme_list_final):\n",
    "    increment = 1\n",
    "    prev_viseme = viseme_list_final[i]\n",
    "    # if the previous instance of the current viseme is not -1\n",
    "    if prev_slider_dict[viseme_list_final[i]] != -1:\n",
    "        current_interval = viseme_intervals_final[i]\n",
    "        prev_interval = viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]]\n",
    "        if (current_interval[1][0] >= prev_interval[2][0] and current_interval[0][0] <= prev_interval[3][0]):\n",
    "            interval = prev_interval[:-1] + current_interval[1:]\n",
    "            viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]] = interval\n",
    "        elif (current_interval[1][0] <= prev_interval[2][0]):\n",
    "            interval = prev_interval[0:-2] + current_interval[1:]\n",
    "            viseme_intervals_final_final[prev_slider_dict[viseme_list_final[i]]] = interval\n",
    "        else:\n",
    "            viseme_list_final_final.append(viseme_list_final[i])\n",
    "            viseme_intervals_final_final.append(viseme_intervals_final[i])\n",
    "                \n",
    "    else:        \n",
    "        viseme_list_final_final.append(viseme_list_final[i])\n",
    "        viseme_intervals_final_final.append(viseme_intervals_final[i])\n",
    "        \n",
    "    prev_slider_dict[viseme_list_final[i]] = len(viseme_list_final_final) - 1\n",
    "    i = i + increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a: Jali parameter version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jali_parameters\n",
    "DEFALT_JALI_VAL = 6\n",
    "\n",
    "jaw_ctrl_pts = []\n",
    "lip_ctrl_pts = []\n",
    "\n",
    "# get information on the frequency\n",
    "freq = lyric.pitch.selected_array[\"frequency\"]\n",
    "xs = lyric.xs\n",
    "freq[freq == 0] = np.nan\n",
    "mask = np.isnan(freq)\n",
    "freq[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), freq[~mask])\n",
    "freq = savgol_filter(freq, 21, 3)\n",
    "f = interp1d(xs, freq, kind=\"linear\")\n",
    "for sentence in sentences:\n",
    "    all_pitch_intervals_slope = []\n",
    "    all_pitch_intervals_time = []\n",
    "    si = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    fs = f(np.arange(si[0], min(si[1], xs[-1]), 0.01))\n",
    "    max_fs = fs.max()\n",
    "    min_fs = fs.min()\n",
    "    # the starting point is always at \n",
    "    current_freq = fs[0]\n",
    "    jaw_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, 6 + (current_freq-min_fs)/(max_fs-min_fs) * 4]\n",
    "    jaw_ctrl_pts.append(jaw_ctrl_pt_0)\n",
    "    lip_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, 6 + (current_freq-min_fs)/(max_fs-min_fs) * 4]\n",
    "    lip_ctrl_pts.append(lip_ctrl_pt_0)\n",
    "    prev_voice_type = \"chest\"\n",
    "    for phone in sentence:\n",
    "        if len(lyric.pitch_slopes[phone]) == 0 and len(lyric.voice_quality_lists[phone]) > 0:\n",
    "            pitch_interval_time = lyric.phoneme_intervals[phone]\n",
    "            if lyric.voice_quality_lists[phone][0] == \"head\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = f(pitch_interval_time[0])\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[1])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"head\"\n",
    "            elif lyric.voice_quality_lists[phone][0] == \"belt\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0])\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], new_lip_val])\n",
    "                if prev_voice_type == \"head\":\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"belt\"\n",
    "        for i in range(0, len(lyric.pitch_slopes[phone])):\n",
    "            pitch_interval_time = lyric.pitch_intervals[phone][i]\n",
    "            if lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"head\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"head\"\n",
    "            elif lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"belt\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], new_lip_val])\n",
    "                if prev_voice_type == \"head\":\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"belt\"\n",
    "            elif lyric.pitch_slopes[phone][i] > 0 and lyric.voice_quality_lists[phone][i] == \"chest\":\n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], new_jaw_val])\n",
    "                if prev_voice_type == \"belt\":\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "                    lip_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "#                 if prev_voice_type == \"head\":\n",
    "#                     jaw_ctrl_pts.append([pitch_interval_time[0], DEFALT_JALI_VAL])\n",
    "#                     jaw_ctrl_pts.append([pitch_interval_time[1], DEFALT_JALI_VAL])\n",
    "                prev_voice_type = \"chest\"\n",
    "            elif lyric.pitch_slopes[phone][i] < 0: # these are opportunities to bring \n",
    "                # the pitch at the beginning of the intervals\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                # pitch at the end of the interval (also updates it)\n",
    "                current_freq = f(pitch_interval_time[0]) + lyric.pitch_slopes[phone][i] * (pitch_interval_time[1] - pitch_interval_time[0])\n",
    "                new_jaw_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                new_lip_val = DEFALT_JALI_VAL + (current_freq - min_fs)/(max_fs-min_fs) * 4\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], max(new_jaw_val, DEFALT_JALI_VAL)])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], max(new_lip_val, DEFALT_JALI_VAL)])\n",
    "                prev_voice_type = lyric.voice_quality_intervals[phone][i]\n",
    "            elif ((prev_voice_type == \"head\" and lyric.voice_quality_intervals[phone][i] == \"belt\") or \n",
    "                  (prev_voice_type == \"belt\" and lyric.voice_quality_intervals[phone][i] == \"head\")):\n",
    "                prev_jaw_val = jaw_ctrl_pts[-1][1]\n",
    "                prev_lip_val = lip_ctrl_pts[-1][1]\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[0], prev_lip_val])\n",
    "                jaw_ctrl_pts.append([pitch_interval_time[1], prev_lip_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[0], prev_jaw_val])\n",
    "                lip_ctrl_pts.append([pitch_interval_time[1], prev_jaw_val])\n",
    "                prev_voice_type = lyric.voice_quality_intervals[phone][i]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a: Jali parameter version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.SongDataStructure import Minimal_song_data_structure\n",
    "dir = \"E:/MASC/Structured_data/rolling_in_the_deep_adele\"\n",
    "file_name_template = \"audio\"\n",
    "min_lyric = Minimal_song_data_structure(os.path.join(dir, file_name_template+\".wav\"), \n",
    "                                       os.path.join(dir, file_name_template+\".txt\"), \n",
    "                                       os.path.join(dir, \"audio_full.TextGrid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value_stat():\n",
    "    def __init__(self, x : np.array, y : np.array):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mean = y.mean()\n",
    "        self.std = y.std()\n",
    "        self.min = y.min()\n",
    "        self.max = y.max()\n",
    "        q = np.nanpercentile(y, [2, 98])\n",
    "        self.min_2 = q[0]\n",
    "        self.max_98 = q[1]\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot(self.x, np.ones(self.x.shape)*self.mean)\n",
    "        plt.plot(self.x, self.y)\n",
    "    def thresholding(self, threshold):\n",
    "        self.y[self.y <= threshold] = np.nan\n",
    "        mask = np.isnan(self.y) \n",
    "        self.y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), self.y[~mask])\n",
    "        self.y = savgol_filter(self.y, 21, 3)\n",
    "        self.y = interp1d(self.x, self.y, kind=\"linear\")(self.x)\n",
    "        \n",
    "        self.mean = self.y.mean()\n",
    "        self.std = self.y.std()\n",
    "        self.min = self.y.min()\n",
    "        self.max = self.y.max()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute coarse intervals from fine ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = lyric.voice_quality_intervals\n",
    "traits = lyric.voice_quality_lists\n",
    "def compute_coarse_intervals(traits, interval):\n",
    "    new_intervals = []\n",
    "    new_traits = []\n",
    "    for i in range(0, len(intervals)):\n",
    "        new_interval = []\n",
    "        new_trait = []\n",
    "        interval = intervals[i]\n",
    "        trait = traits[i]\n",
    "        if len(trait) > 1:\n",
    "            prev_trait = trait[0]\n",
    "            prev_index = 0\n",
    "            for k in range(1, len(trait)):\n",
    "                if trait[k] == prev_trait and k == len(trait)-1:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k][1]])\n",
    "                elif trait[k] == prev_trait:\n",
    "                    continue\n",
    "                elif trait[k] != prev_trait:\n",
    "                    new_trait.append(prev_trait)\n",
    "                    new_interval.append([interval[prev_index][0], interval[k-1][1]])\n",
    "                    prev_trait = trait[k]\n",
    "            new_traits.append(new_trait)\n",
    "            new_intervals.append(new_interval)\n",
    "        else:\n",
    "            new_traits.append(trait)\n",
    "            new_intervals.append(interval)\n",
    "    return new_traits, new_intervals\n",
    "new_traits, new_interval = compute_coarse_intervals(traits, intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather statistics from the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jali_parameters\n",
    "DEFALT_JALI_VAL = 5\n",
    "\n",
    "# get information on the frequency\n",
    "freq = min_lyric.pitch_arr\n",
    "intensity = min_lyric.intensity_arr[:,0]\n",
    "xs = lyric.xs\n",
    "freq[freq == 0] = np.nan\n",
    "mask = np.isnan(freq) \n",
    "freq[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), freq[~mask])\n",
    "freq = savgol_filter(freq, 21, 3)\n",
    "f = interp1d(xs, freq, kind=\"linear\")\n",
    "\n",
    "xs = min_lyric.intensity.xs()\n",
    "intensity[intensity == 0] = np.nan\n",
    "mask = np.isnan(intensity)\n",
    "intensity[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), intensity[~mask])\n",
    "intensity = savgol_filter(intensity, 21, 3)\n",
    "I = interp1d(xs, intensity, kind=\"linear\")\n",
    "\n",
    "song_stat_f = Value_stat(min_lyric.pitch.xs(), freq)\n",
    "song_stat_I = Value_stat(min_lyric.intensity.xs(), intensity)\n",
    "sentence_stats_f = []\n",
    "sentence_stats_I = []\n",
    "\n",
    "max_min_f = 0\n",
    "max_min_I = 0\n",
    "for sentence in sentences: \n",
    "    # get the interval of the current sentence\n",
    "    s_interval = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    s_xs = np.arange(s_interval[0], min(s_interval[1], min_lyric.intensity.xs()[-1]), 0.01)\n",
    "    s_intensity = I(s_xs)\n",
    "    s_freq = f(s_xs)\n",
    "    stat_i = Value_stat(s_xs, s_intensity)\n",
    "    stat_f = Value_stat(s_xs, s_freq)\n",
    "    max_min_I = max(stat_i.min_2, max_min_I)\n",
    "    max_min_f = max(stat_f.min_2, max_min_f)\n",
    "    sentence_stats_f.append(stat_f)\n",
    "    sentence_stats_I.append(stat_i)\n",
    "for s in sentence_stats_f:\n",
    "    s.thresholding(max_min_f)\n",
    "for s in sentence_stats_I:\n",
    "    s.thresholding(max_min_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [0.0840625, 0.1000625]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_1304/1625104010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0minterval_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphoneme_intervals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlyric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphoneme_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mxs_phone_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mfreq_phone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs_phone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs_phone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "jaw_ctrl_pts = []\n",
    "lip_ctrl_pts = []\n",
    "dimple_ctrl_pts = []\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    all_pitch_intervals_slope = []\n",
    "    all_pitch_intervals_time = []\n",
    "    si = [lyric.phoneme_intervals[sentence[0]][0],\n",
    "          lyric.phoneme_intervals[sentence[-1]][1]]\n",
    "    # the starting point is always at \n",
    "    current_freq = fs[0]\n",
    "    jaw_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, DEFALT_JALI_VAL]\n",
    "    jaw_ctrl_pts.append(jaw_ctrl_pt_0)\n",
    "    lip_ctrl_pt_0 = [lyric.phoneme_intervals[sentence[0]][0]-0.02, DEFALT_JALI_VAL]\n",
    "    lip_ctrl_pts.append(lip_ctrl_pt_0)\n",
    "    for phone in sentence:\n",
    "        interval_time = lyric.phoneme_intervals[phone]\n",
    "        print(lyric.phoneme_list[phone], interval_time)\n",
    "        xs_phone_freq = np.arange(interval_time[0], min(interval_time[1]), 0.01)\n",
    "        freq_phone = f(xs_phone)\n",
    "        print(xs_phone.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_ctrl_pts = []\n",
    "for k in lyric.vibrato_intervals:\n",
    "    if len(k) > 0:\n",
    "        for m in k:\n",
    "            vib_ctrl_pts.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\"viseme\":[viseme_list_final_final, viseme_intervals_final_final],\n",
    "        \"brow\":[brow_movement, brow_ctrl_points, finer_brow_raise_ctrl_points, finer_brow_furrow_ctrl_points],\n",
    "        \"blink\":[eye_movement, eye_ctrl_points],\n",
    "        \"jaw\":jaw_ctrl_pts,\n",
    "        \"lip\":lip_ctrl_pts, \n",
    "        \"vib\":vib_ctrl_pts}\n",
    "jsonoutput = json.dumps(output)\n",
    "with open(os.path.join(dir, file_name_template+'_animation_data.json'), 'w') as outfile:\n",
    "    json.dump(jsonoutput, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple head movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_shake(t_start, t_end, prev_motion_x, prev_motions_y):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the thing into sentence structures\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for i in range(0, len(lyric.phoneme_list)):\n",
    "    if lyric.phoneme_list[i] == \"EOS_tag\":\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    else:\n",
    "        current_sentence.append(i)\n",
    "        if i == len(lyric.phoneme_list) - 1:\n",
    "            sentences.append(current_sentence)\n",
    "sentences = sentences[1:] \n",
    "\n",
    "x_dir_head = []\n",
    "y_dir_head = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying eye brow movements using facial landmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.facial_landmarking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'E:/facial_data_analysis_videos/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_2640/1733076059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvideo_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"video.mp4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"E:/facial_data_analysis_videos/1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m extract_landmarks_media_pipe(video_title[0],\n\u001b[0m\u001b[0;32m      4\u001b[0m                          video_path[0], save_annotated_video=True)\n",
      "\u001b[1;32m~\\Desktop\\jali_sing\\util\\facial_landmarking.py\u001b[0m in \u001b[0;36mextract_landmarks_media_pipe\u001b[1;34m(input_video, input_dir, show_annotated_video, show_normalized_pts, save_annotated_video, tolerance)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#     return output_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mget_audio_from_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_video\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# set up cv2 object for querying images from video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\jali_sing\\util\\ioUtil.py\u001b[0m in \u001b[0;36mget_audio_from_video\u001b[1;34m(file_name, video_folder_path, target_fps, remove)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_audio_from_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdir_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_folder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The directory is empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'E:/facial_data_analysis_videos/1'"
     ]
    }
   ],
   "source": [
    "video_title = [\"video.mp4\"]\n",
    "video_path = [\"E:/facial_data_analysis_videos/1\"]\n",
    "extract_landmarks_media_pipe(video_title[0],\n",
    "                         video_path[0], save_annotated_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jali_sing",
   "language": "python",
   "name": "jali_sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
