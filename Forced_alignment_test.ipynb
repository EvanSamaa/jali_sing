{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import decimate\n",
    "import torch\n",
    "import plla_tisvs.data as data\n",
    "import plla_tisvs.model as model\n",
    "import plla_tisvs.utils as utils\n",
    "import plla_tisvs.testx as testx\n",
    "import json\n",
    "from plla_tisvs.estimate_alignment import optimal_alignment_path, compute_phoneme_onsets\n",
    "from plla_tisvs.preprocessing_input import Custom_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"./plla_tisvs/dicts\"\n",
    "model_path = './plla_tisvs/trained_models/{}'.format(\"JOINT3\")\n",
    "phoneme_dict_path = \"cmu_word2cmu_phoneme_extra.pickle\"\n",
    "audio_paths = [\"E:/Speech_data_set/alignment_test/rolling_in_the_deep.wav\"]\n",
    "transcript_paths = [\"E:/Speech_data_set/alignment_test/rolling_in_the_deep.txt\"]\n",
    "\n",
    "# parse data\n",
    "data_parser = Custom_data_set(dict_path, phoneme_dict_path)\n",
    "audio, phoneme_idx, phoneme_list_full, word_list = data_parser.parse(audio_paths[0], transcript_paths[0])\n",
    "\n",
    "# ------------- remove this if it starts causing error ------------- \n",
    "# audio = torch.unsqueeze(audio, 0)\n",
    "# audio = audio.tile((1, 2, 1))\n",
    "# print(audio.shape)\n",
    "# ------------- remove this if it starts causing error ------------- \n",
    "\n",
    "# load model\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(\"Device:\", device)\n",
    "target = 'vocals'\n",
    "\n",
    "# load model\n",
    "model_to_test = testx.load_model(target, model_path, device)\n",
    "model_to_test.return_alphas = True\n",
    "model_to_test.eval()\n",
    "\n",
    "# load model config\n",
    "with open(os.path.join(model_path, target + '.json'), 'r') as stream:\n",
    "    config = json.load(stream)\n",
    "    samplerate = config['args']['samplerate']\n",
    "    text_units = config['args']['text_units']\n",
    "    nfft = config['args']['nfft']\n",
    "    nhop = config['args']['nhop']\n",
    "\n",
    "with torch.no_grad():\n",
    "    vocals_estimate, alphas, scores = model_to_test((audio, phoneme_idx))\n",
    "\n",
    "optimal_path_scores = optimal_alignment_path(scores, mode='max_numpy', init=200)\n",
    "\n",
    "phoneme_onsets = compute_phoneme_onsets(optimal_path_scores, hop_length=nhop, sampling_rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_list = data_parser.get_phonemes(phoneme_idx[0])\n",
    "length_of_list = len(phoneme_onsets) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', 'W', '>', 'IY', '>', 'K', '>', 'UH', '>', 'D', '>', 'HH', '>', 'AE', '>', 'V', '>', 'HH', '>', 'AE', '>', 'D', '>', 'IH', '>', 'T', '>', 'AO', '>', 'L', '>', 'R', '>', 'OW', '>', 'L', '>', 'IH', '>', 'NG', '>', 'IH', '>', 'N', '>', 'DH', '>', 'AH', '>', 'D', '>', 'IY', '>', 'P', '>', 'Y', '>', 'UW', '>', 'HH', '>', 'AE', '>', 'D', '>', 'M', '>', 'AY', '>', 'HH', '>', 'AA', '>', 'R', '>', 'T', '>', 'IH', '>', 'N', '>', 'S', '>', 'AY', '>', 'D', '>', 'AH', '>', 'V', '>', 'Y', '>', 'AO', '>', 'R', '>', 'HH', '>', 'AE', '>', 'N', '>', 'D', '>', 'Z', '>', 'AH', '>', 'N', '>', 'D', '>', 'Y', '>', 'UW', '>', 'P', '>', 'L', '>', 'EY', '>', 'D', '>', 'IH', '>', 'T', '>', 'T', '>', 'UW', '>', 'DH', '>', 'AH', '>', 'B', '>', 'IY', '>', 'T', '$']\n"
     ]
    }
   ],
   "source": [
    "print(phoneme_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W \t 0.016 0.112\n",
      "> \t 0.112 0.128\n",
      "IY \t 0.128 0.352\n",
      "> \t 0.352 0.368\n",
      "K \t 0.368 0.4\n",
      "> \t 0.4 0.416\n",
      "UH \t 0.416 0.848\n",
      "> \t 0.848 0.864\n",
      "D \t 0.864 0.912\n",
      "> \t 0.912 0.928\n",
      "HH \t 0.928 0.944\n",
      "> \t 0.944 0.96\n",
      "AE \t 0.96 1.024\n",
      "> \t 1.024 1.04\n",
      "V \t 1.04 1.056\n",
      "> \t 1.056 1.072\n",
      "HH \t 1.072 1.216\n",
      "> \t 1.216 1.232\n",
      "AE \t 1.232 1.28\n",
      "> \t 1.28 1.296\n",
      "D \t 1.296 1.312\n",
      "> \t 1.312 1.344\n",
      "IH \t 1.344 1.36\n",
      "> \t 1.36 1.408\n",
      "T \t 1.408 1.424\n",
      "> \t 1.424 1.52\n",
      "AO \t 1.52 5.616\n",
      "> \t 5.616 5.632\n",
      "L \t 5.632 5.648\n",
      "> \t 5.648 5.664\n",
      "R \t 5.664 5.84\n",
      "> \t 5.84 5.888\n",
      "OW \t 5.888 5.952\n",
      "> \t 5.952 5.984\n",
      "L \t 5.984 6.336\n",
      "> \t 6.336 6.448\n",
      "IH \t 6.448 6.608\n",
      "> \t 6.608 6.624\n",
      "NG \t 6.624 6.64\n",
      "> \t 6.64 6.688\n",
      "IH \t 6.688 9.28\n",
      "> \t 9.28 9.296\n",
      "N \t 9.296 9.408\n",
      "> \t 9.408 9.424\n",
      "DH \t 9.424 9.44\n",
      "> \t 9.44 9.456\n",
      "AH \t 9.456 9.504\n",
      "> \t 9.504 9.52\n",
      "D \t 9.52 9.712\n",
      "> \t 9.712 9.728\n",
      "IY \t 9.728 9.744\n",
      "> \t 9.744 9.76\n",
      "P \t 9.76 9.776\n",
      "> \t 9.776 9.792\n",
      "Y \t 9.792 9.808\n",
      "> \t 9.808 9.824\n",
      "UW \t 9.824 9.84\n",
      "> \t 9.84 9.856\n",
      "HH \t 9.856 9.872\n",
      "> \t 9.872 9.888\n",
      "AE \t 9.888 9.904\n",
      "> \t 9.904 9.92\n",
      "D \t 9.92 9.936\n",
      "> \t 9.936 9.952\n",
      "M \t 9.952 9.968\n",
      "> \t 9.968 9.984\n",
      "AY \t 9.984 10.0\n",
      "> \t 10.0 10.016\n",
      "HH \t 10.016 10.032\n",
      "> \t 10.032 10.048\n",
      "AA \t 10.048 14.224\n",
      "> \t 14.224 14.24\n",
      "R \t 14.24 14.256\n",
      "> \t 14.256 14.272\n",
      "T \t 14.272 14.288\n",
      "> \t 14.288 14.304\n",
      "IH \t 14.304 14.32\n",
      "> \t 14.32 14.336\n",
      "N \t 14.336 14.352\n",
      "> \t 14.352 14.368\n",
      "S \t 14.368 15.008\n",
      "> \t 15.008 15.024\n",
      "AY \t 15.024 15.04\n",
      "> \t 15.04 15.056\n",
      "D \t 15.056 15.2\n",
      "> \t 15.2 15.216\n",
      "AH \t 15.216 15.232\n",
      "> \t 15.232 15.248\n",
      "V \t 15.248 15.264\n",
      "> \t 15.264 15.28\n",
      "Y \t 15.28 15.296\n",
      "> \t 15.296 15.312\n",
      "AO \t 15.312 16.896\n",
      "> \t 16.896 16.912\n",
      "R \t 16.912 16.928\n",
      "> \t 16.928 16.944\n",
      "HH \t 16.944 16.96\n",
      "> \t 16.96 16.976\n",
      "AE \t 16.976 16.992\n",
      "> \t 16.992 17.008\n",
      "N \t 17.008 17.024\n",
      "> \t 17.024 17.04\n",
      "D \t 17.04 17.056\n",
      "> \t 17.056 17.136\n",
      "Z \t 17.136 17.776\n",
      "> \t 17.776 17.792\n",
      "AH \t 17.792 17.808\n",
      "> \t 17.808 17.824\n",
      "N \t 17.824 17.84\n",
      "> \t 17.84 17.856\n",
      "D \t 17.856 17.872\n",
      "> \t 17.872 17.888\n",
      "Y \t 17.888 17.904\n",
      "> \t 17.904 17.92\n",
      "UW \t 17.92 18.016\n",
      "> \t 18.016 18.032\n",
      "P \t 18.032 18.048\n",
      "> \t 18.048 18.064\n",
      "L \t 18.064 18.08\n",
      "> \t 18.08 18.096\n",
      "EY \t 18.096 18.128\n",
      "> \t 18.128 18.144\n",
      "D \t 18.144 18.16\n",
      "> \t 18.16 18.176\n",
      "IH \t 18.176 18.192\n",
      "> \t 18.192 18.208\n",
      "T \t 18.208 18.224\n",
      "> \t 18.224 18.24\n",
      "T \t 18.24 18.256\n",
      "> \t 18.256 18.272\n",
      "UW \t 18.272 18.288\n",
      "> \t 18.288 18.304\n",
      "DH \t 18.304 18.32\n",
      "> \t 18.32 18.336\n",
      "AH \t 18.336 18.352\n",
      "> \t 18.352 18.368\n",
      "B \t 18.368 18.384\n",
      "> \t 18.384 18.4\n",
      "IY \t 18.4 19.952\n",
      "> \t 19.952 19.968\n",
      "T \t 19.968 19.984\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, length_of_list):\n",
    "    print(phoneme_list[i], '\\t' ,phoneme_onsets[i], phoneme_onsets[i+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfa align E:/Speech_data_set/alignment_test C:/Users/evansamaa/Desktop/jali_sing/util/mfa_english_dict.txt english E:/Speech_data_set/alignment_test/i_dont_love_you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_path = \"E:/Speech_data_set/alignment_test\"\n",
    "output_path = \"E:/Speech_data_set/alignment_test/i_dont_love_you\"\n",
    "dict_path = \"C:/Users/evansamaa/Desktop/jali_sing/util/mfa_english_dict.txt\"\n",
    "command_context = \"mfa align {} {} {} {}\".format(data_set_path, dict_path, \"english\", output_path)\n",
    "\n",
    "print(command_context)\n",
    "os.system(command_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = textgrids.TextGrid(output_path + \"/child_in_time_1_for_mfa.TextGrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interval text=\"\" xmin=0.0 xmax=0.55>\n",
      "<Interval text=\"S\" xmin=0.55 xmax=0.83>\n",
      "<Interval text=\"W\" xmin=0.83 xmax=0.95>\n",
      "<Interval text=\"IY1\" xmin=0.95 xmax=1.22>\n",
      "<Interval text=\"T\" xmin=1.22 xmax=1.37>\n",
      "<Interval text=\"CH\" xmin=1.37 xmax=1.55>\n",
      "<Interval text=\"AY1\" xmin=1.55 xmax=1.93>\n",
      "<Interval text=\"L\" xmin=1.93 xmax=2.0>\n",
      "<Interval text=\"D\" xmin=2.0 xmax=2.07>\n",
      "<Interval text=\"IH0\" xmin=2.07 xmax=2.1>\n",
      "<Interval text=\"N\" xmin=2.1 xmax=2.18>\n",
      "<Interval text=\"T\" xmin=2.18 xmax=2.31>\n",
      "<Interval text=\"AY1\" xmin=2.31 xmax=2.85>\n",
      "<Interval text=\"M\" xmin=2.85 xmax=2.88>\n",
      "<Interval text=\"Y\" xmin=2.88 xmax=2.91>\n",
      "<Interval text=\"UW1\" xmin=2.91 xmax=2.94>\n",
      "<Interval text=\"L\" xmin=2.94 xmax=2.97>\n",
      "<Interval text=\"S\" xmin=2.97 xmax=3.0>\n",
      "<Interval text=\"IY1\" xmin=3.0 xmax=4.39>\n",
      "<Interval text=\"\" xmin=4.39 xmax=4.89>\n",
      "<Interval text=\"DH\" xmin=4.89 xmax=4.99>\n",
      "<Interval text=\"AH1\" xmin=4.99 xmax=5.96>\n",
      "<Interval text=\"\" xmin=5.96 xmax=5.99>\n",
      "<Interval text=\"L\" xmin=5.99 xmax=6.28>\n",
      "<Interval text=\"AY1\" xmin=6.28 xmax=7.4>\n",
      "<Interval text=\"N\" xmin=7.4 xmax=8.41>\n",
      "<Interval text=\"\" xmin=8.41 xmax=9.05>\n",
      "<Interval text=\"DH\" xmin=9.05 xmax=9.09>\n",
      "<Interval text=\"AH1\" xmin=9.09 xmax=9.18>\n",
      "<Interval text=\"\" xmin=9.18 xmax=9.21>\n",
      "<Interval text=\"L\" xmin=9.21 xmax=9.38>\n",
      "<Interval text=\"AY1\" xmin=9.38 xmax=9.72>\n",
      "<Interval text=\"N\" xmin=9.72 xmax=9.87>\n",
      "<Interval text=\"DH\" xmin=9.87 xmax=9.91>\n",
      "<Interval text=\"AE1\" xmin=9.91 xmax=9.94>\n",
      "<Interval text=\"T\" xmin=9.94 xmax=10.01>\n",
      "<Interval text=\"S\" xmin=10.01 xmax=10.09>\n",
      "<Interval text=\"D\" xmin=10.09 xmax=10.21>\n",
      "<Interval text=\"R\" xmin=10.21 xmax=10.26>\n",
      "<Interval text=\"AO1\" xmin=10.26 xmax=10.61>\n",
      "<Interval text=\"N\" xmin=10.61 xmax=10.7>\n",
      "<Interval text=\"B\" xmin=10.7 xmax=10.73>\n",
      "<Interval text=\"IH0\" xmin=10.73 xmax=10.78>\n",
      "<Interval text=\"T\" xmin=10.78 xmax=10.99>\n",
      "<Interval text=\"W\" xmin=10.99 xmax=11.06>\n",
      "<Interval text=\"IY1\" xmin=11.06 xmax=11.11>\n",
      "<Interval text=\"N\" xmin=11.11 xmax=12.31>\n",
      "<Interval text=\"\" xmin=12.31 xmax=13.23>\n",
      "<Interval text=\"G\" xmin=13.23 xmax=13.35>\n",
      "<Interval text=\"UH1\" xmin=13.35 xmax=13.58>\n",
      "<Interval text=\"D\" xmin=13.58 xmax=13.62>\n",
      "<Interval text=\"AH0\" xmin=13.62 xmax=15.54>\n",
      "<Interval text=\"N\" xmin=15.54 xmax=15.57>\n",
      "<Interval text=\"D\" xmin=15.57 xmax=15.6>\n",
      "<Interval text=\"B\" xmin=15.6 xmax=15.63>\n",
      "<Interval text=\"AE1\" xmin=15.63 xmax=15.66>\n",
      "<Interval text=\"D\" xmin=15.66 xmax=16.28>\n",
      "<Interval text=\"\" xmin=16.28 xmax=16.77>\n",
      "<Interval text=\"S\" xmin=16.77 xmax=17.32>\n",
      "<Interval text=\"IY1\" xmin=17.32 xmax=17.4>\n",
      "<Interval text=\"DH\" xmin=17.4 xmax=17.44>\n",
      "<Interval text=\"AH1\" xmin=17.44 xmax=19.87>\n",
      "<Interval text=\"\" xmin=19.87 xmax=19.9>\n",
      "<Interval text=\"B\" xmin=19.9 xmax=19.93>\n",
      "<Interval text=\"L\" xmin=19.93 xmax=19.97>\n",
      "<Interval text=\"AY1\" xmin=19.97 xmax=20.0>\n",
      "<Interval text=\"N\" xmin=20.0 xmax=20.04>\n",
      "<Interval text=\"D\" xmin=20.04 xmax=20.07>\n",
      "<Interval text=\"M\" xmin=20.07 xmax=20.11>\n",
      "<Interval text=\"AE1\" xmin=20.11 xmax=20.23>\n",
      "<Interval text=\"N\" xmin=20.23 xmax=20.26>\n",
      "<Interval text=\"\" xmin=20.26 xmax=20.89>\n",
      "<Interval text=\"SH\" xmin=20.89 xmax=21.32>\n",
      "<Interval text=\"UW1\" xmin=21.32 xmax=21.63>\n",
      "<Interval text=\"T\" xmin=21.63 xmax=21.68>\n",
      "<Interval text=\"IH0\" xmin=21.68 xmax=21.76>\n",
      "<Interval text=\"NG\" xmin=21.76 xmax=21.79>\n",
      "<Interval text=\"AE1\" xmin=21.79 xmax=22.34>\n",
      "<Interval text=\"T\" xmin=22.34 xmax=22.44>\n",
      "<Interval text=\"DH\" xmin=22.44 xmax=22.47>\n",
      "<Interval text=\"AH1\" xmin=22.47 xmax=24.29>\n",
      "<Interval text=\"\" xmin=24.29 xmax=24.32>\n",
      "<Interval text=\"W\" xmin=24.32 xmax=24.35>\n",
      "<Interval text=\"ER1\" xmin=24.35 xmax=24.39>\n",
      "<Interval text=\"L\" xmin=24.39 xmax=24.46>\n",
      "<Interval text=\"D\" xmin=24.46 xmax=24.49>\n",
      "<Interval text=\"\" xmin=24.49 xmax=25.22>\n",
      "<Interval text=\"B\" xmin=25.22 xmax=25.32>\n",
      "<Interval text=\"UH1\" xmin=25.32 xmax=25.38>\n",
      "<Interval text=\"L\" xmin=25.38 xmax=25.72>\n",
      "<Interval text=\"AH0\" xmin=25.72 xmax=25.77>\n",
      "<Interval text=\"T\" xmin=25.77 xmax=25.83>\n",
      "<Interval text=\"S\" xmin=25.83 xmax=25.93>\n",
      "<Interval text=\"F\" xmin=25.93 xmax=26.06>\n",
      "<Interval text=\"L\" xmin=26.06 xmax=26.11>\n",
      "<Interval text=\"AY1\" xmin=26.11 xmax=26.6>\n",
      "<Interval text=\"IH0\" xmin=26.6 xmax=26.81>\n",
      "<Interval text=\"NG\" xmin=26.81 xmax=28.25>\n",
      "<Interval text=\"\" xmin=28.25 xmax=28.94>\n",
      "<Interval text=\"spn\" xmin=28.94 xmax=29.92>\n",
      "<Interval text=\"T\" xmin=29.92 xmax=29.95>\n",
      "<Interval text=\"EY1\" xmin=29.95 xmax=29.98>\n",
      "<Interval text=\"K\" xmin=29.98 xmax=30.01>\n",
      "<Interval text=\"IH0\" xmin=30.01 xmax=30.14>\n",
      "<Interval text=\"NG\" xmin=30.14 xmax=30.33>\n",
      "<Interval text=\"T\" xmin=30.33 xmax=30.54>\n",
      "<Interval text=\"OW1\" xmin=30.54 xmax=31.33>\n",
      "<Interval text=\"L\" xmin=31.33 xmax=32.52>\n",
      "<Interval text=\"\" xmin=32.52 xmax=32.54>\n"
     ]
    }
   ],
   "source": [
    "# print(grid.items())\n",
    "# Get the phones\n",
    "num_of_intervals = len(grid[\"phones\"])\n",
    "for i in range(0, num_of_intervals):\n",
    "    print(grid[\"phones\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Interval text=\"\" xmin=0.0 xmax=0.55>, <Interval text=\"sweet\" xmin=0.55 xmax=1.37>, <Interval text=\"child\" xmin=1.37 xmax=2.07>, <Interval text=\"in\" xmin=2.07 xmax=2.18>, <Interval text=\"time\" xmin=2.18 xmax=2.88>, <Interval text=\"you'll\" xmin=2.88 xmax=2.97>, <Interval text=\"see\" xmin=2.97 xmax=4.39>, <Interval text=\"\" xmin=4.39 xmax=4.89>, <Interval text=\"the\" xmin=4.89 xmax=5.96>, <Interval text=\"\" xmin=5.96 xmax=5.99>, <Interval text=\"line\" xmin=5.99 xmax=8.41>, <Interval text=\"\" xmin=8.41 xmax=9.05>, <Interval text=\"the\" xmin=9.05 xmax=9.18>, <Interval text=\"\" xmin=9.18 xmax=9.21>, <Interval text=\"line\" xmin=9.21 xmax=9.87>, <Interval text=\"that's\" xmin=9.87 xmax=10.09>, <Interval text=\"drawn\" xmin=10.09 xmax=10.7>, <Interval text=\"between\" xmin=10.7 xmax=12.31>, <Interval text=\"\" xmin=12.31 xmax=13.23>, <Interval text=\"good\" xmin=13.23 xmax=13.62>, <Interval text=\"and\" xmin=13.62 xmax=15.6>, <Interval text=\"bad\" xmin=15.6 xmax=16.28>, <Interval text=\"\" xmin=16.28 xmax=16.77>, <Interval text=\"see\" xmin=16.77 xmax=17.4>, <Interval text=\"the\" xmin=17.4 xmax=19.87>, <Interval text=\"\" xmin=19.87 xmax=19.9>, <Interval text=\"blind\" xmin=19.9 xmax=20.07>, <Interval text=\"man\" xmin=20.07 xmax=20.26>, <Interval text=\"\" xmin=20.26 xmax=20.89>, <Interval text=\"shooting\" xmin=20.89 xmax=21.79>, <Interval text=\"at\" xmin=21.79 xmax=22.44>, <Interval text=\"the\" xmin=22.44 xmax=24.29>, <Interval text=\"\" xmin=24.29 xmax=24.32>, <Interval text=\"world\" xmin=24.32 xmax=24.49>, <Interval text=\"\" xmin=24.49 xmax=25.22>, <Interval text=\"bullets\" xmin=25.22 xmax=25.93>, <Interval text=\"flying\" xmin=25.93 xmax=28.25>, <Interval text=\"\" xmin=28.25 xmax=28.94>, <Interval text=\"ohh\" xmin=28.94 xmax=29.92>, <Interval text=\"taking\" xmin=29.92 xmax=30.33>, <Interval text=\"toll\" xmin=30.33 xmax=32.52>, <Interval text=\"\" xmin=32.52 xmax=32.54>]\n"
     ]
    }
   ],
   "source": [
    "print(grid[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_alignment(phoneme_onsets, phoneme_list_full):\n",
    "    word_durations = []\n",
    "    pointer_i = 0 # this one is for the phoneme_list_full\n",
    "    pointer_j = 0 # this one is for phoneme_onsets\n",
    "    begin = phoneme_onsets[pointer_j]\n",
    "    phone_copy = ['EOW'] + phoneme_list_full\n",
    "    while pointer_j < len(phoneme_onsets):\n",
    "        if phone_copy[pointer_i] == \"EOW\":\n",
    "            word_durations.append([begin, phoneme_onsets[pointer_j]])\n",
    "            if pointer_j + 1 == len(phoneme_onsets):\n",
    "                break\n",
    "            if phoneme_onsets[min(pointer_j + 1, len(phoneme_onsets)-1)] != \"<\":\n",
    "                begin = phoneme_onsets[min(pointer_j + 1, len(phoneme_onsets)-1)]\n",
    "                pointer_i = pointer_i + 2\n",
    "                pointer_j = pointer_j + 1\n",
    "            else:\n",
    "                begin = phoneme_onsets[min(pointer_j + 2, len(phoneme_onsets)-1)]\n",
    "                pointer_i = pointer_i + 3\n",
    "                pointer_j = pointer_j + 2\n",
    "        else:\n",
    "            pointer_i = pointer_i + 1\n",
    "            pointer_j = pointer_j + 1\n",
    "    return word_durations[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', 'W', '>', 'IY', '>', 'K', '>', 'UH', '>', 'D', '>', 'HH', '>', 'AE', '>', 'V', '>', 'HH', '>', 'AE', '>', 'D', '>', 'IH', '>', 'T', '>', 'AO', '>', 'L', '>', 'R', '>', 'OW', '>', 'L', '>', 'IH', '>', 'NG', '>', 'IH', '>', 'N', '>', 'DH', '>', 'AH', '>', 'D', '>', 'IY', '>', 'P', '>', 'Y', '>', 'UW', '>', 'HH', '>', 'AE', '>', 'D', '>', 'M', '>', 'AY', '>', 'HH', '>', 'AA', '>', 'R', '>', 'T', '>', 'IH', '>', 'N', '>', 'S', '>', 'AY', '>', 'D', '>', 'AH', '>', 'V', '>', 'Y', '>', 'AO', '>', 'R', '>', 'HH', '>', 'AE', '>', 'N', '>', 'D', '>', 'Z', '>', 'AH', '>', 'N', '>', 'D', '>', 'Y', '>', 'UW', '>', 'P', '>', 'L', '>', 'EY', '>', 'D', '>', 'IH', '>', 'T', '>', 'T', '>', 'UW', '>', 'DH', '>', 'AH', '>', 'B', '>', 'IY', '>', 'T', '$']\n"
     ]
    }
   ],
   "source": [
    "word_durations = compute_word_alignment(phoneme_onsets, phoneme_list_full)\n",
    "print(phoneme_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs \n",
    "phoneme_list # this is the list of all the phonemes\n",
    "phoneme_onsets # this is the list of all the onsets\n",
    "\n",
    "new_grid = textgrids.TextGrid() # initialize new_textgrid object\n",
    "\n",
    "new_grid.xmin = 0\n",
    "new_grid.xmax = phoneme_onsets[-1]\n",
    "new_grid[\"phones\"] = []\n",
    "for i in range(1, len(phoneme_onsets) - 1):\n",
    "    phoneme = phoneme_list[i]\n",
    "    if phoneme == \">\":\n",
    "        phoneme = \"\"\n",
    "    interval = textgrids.Interval(phoneme, phoneme_onsets[i], phoneme_onsets[i+1])\n",
    "    new_grid[\"phones\"].append(interval)\n",
    "    \n",
    "new_grid[\"words\"] = []\n",
    "for i in range(0, len(word_list)):\n",
    "    interval = textgrids.Interval(word_list[i], word_durations[i][0], word_durations[i][1])\n",
    "    new_grid[\"words\"].append(interval)\n",
    "new_grid.write(output_path + \"/rolling_in_the_deep.TextGrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import norbert\n",
    "import soundfile as sf\n",
    "\n",
    "def istft(X, rate=44100, n_fft=4096, n_hopsize=1024):\n",
    "    t, audio = scipy.signal.istft(\n",
    "        X / (n_fft / 2),\n",
    "        rate,\n",
    "        nperseg=n_fft,\n",
    "        noverlap=n_fft - n_hopsize,\n",
    "        boundary=True\n",
    "    )\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plla_tisvs.preprocessing_input import Custom_data_set\n",
    "from plla_tisvs import testx\n",
    "import numpy as np\n",
    "\n",
    "audio_path_file = \"E:/MASC/voice_seperation_test/child_in_time_raw.wav\"\n",
    "transcript_path = \"E:/MASC/voice_seperation_test/child_in_time_raw.txt\"\n",
    "dict_path = \"./plla_tisvs/dicts\"\n",
    "model_path = './plla_tisvs/trained_models/{}'.format(\"JOINT3\")\n",
    "phoneme_dict_path = \"cmu_word2cmu_phoneme_extra.pickle\"\n",
    "softmask = True\n",
    "niter = 2\n",
    "try:\n",
    "    data_parser = Custom_data_set(dict_path, phoneme_dict_path)\n",
    "except:\n",
    "    dict_path = \".\" + dict_path\n",
    "    data_parser = Custom_data_set(dict_path, phoneme_dict_path)\n",
    "audio, phoneme_idx, phoneme_list_full, word_list = data_parser.parse(audio_path_file,\n",
    "                                                                          transcript_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "target = 'vocals'\n",
    "# load model\n",
    "try:\n",
    "    model_to_test = testx.load_model(target, model_path, device)\n",
    "except:\n",
    "    model_path = \".\" + model_path\n",
    "    model_to_test = testx.load_model(target, model_path, device)\n",
    "model_to_test.eval()\n",
    "model_to_test.return_alphas = True\n",
    "out = model_to_test((audio, phoneme_idx))\n",
    "alphas = out[1].cpu().detach().numpy()\n",
    "Vj = out[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = []\n",
    "# output is nb_frames, nb_samples, nb_channels, nb_bins\n",
    "V.append(Vj[:, 0, ...])  # remove sample dim\n",
    "# source_names += [target]\n",
    "V = np.transpose(np.array(V), (1, 3, 2, 0))\n",
    "\n",
    "X = model_to_test.stft(audio).detach().cpu().numpy()\n",
    "# convert to complex numpy type\n",
    "X = X[..., 0] + X[..., 1] * 1j\n",
    "X = X[0].transpose(2, 1, 0)\n",
    "\n",
    "V = norbert.residual_model(V, X, 1)\n",
    "\n",
    "Y = norbert.wiener(V, X.astype(np.complex128), niter,\n",
    "                   use_softmask=True)\n",
    "\n",
    "estimates = {}\n",
    "for j, name in enumerate([\"vocals\"]):\n",
    "    audio_hat = istft(\n",
    "        Y[..., j].T,\n",
    "        n_fft=model_to_test.stft.n_fft,\n",
    "        n_hopsize=model_to_test.stft.n_hop\n",
    "    )\n",
    "    estimates[name] = audio_hat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(audio_path_file[:-4] + \"_vocals.wav\", estimates['vocals'], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visemenet",
   "language": "python",
   "name": "visemenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
