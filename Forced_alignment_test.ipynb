{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import decimate\n",
    "import torch\n",
    "import plla_tisvs.data as data\n",
    "import plla_tisvs.model as model\n",
    "import plla_tisvs.utils as utils\n",
    "import plla_tisvs.testx as testx\n",
    "import json\n",
    "from plla_tisvs.estimate_alignment import optimal_alignment_path, compute_phoneme_onsets\n",
    "from plla_tisvs.preprocessing_input import Custom_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 717507)\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evansamaa/anaconda3/envs/jali_sing/lib/python3.7/site-packages/torch/functional.py:472: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  ../aten/src/ATen/native/SpectralOps.cpp:664.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"./plla_tisvs/dicts\"\n",
    "model_path = './plla_tisvs/trained_models/{}'.format(\"JOINT3\")\n",
    "phoneme_dict_path = \"cmu_word2cmu_phoneme_extra.pickle\"\n",
    "audio_paths = [\"/Volumes/EVAN_DISK/ten_videos/Child_in_time/Child_in_time_1/audio.wav\"]\n",
    "transcript_paths = [\"/Volumes/EVAN_DISK/ten_videos/Child_in_time/Child_in_time_1/audio.txt\"]\n",
    "\n",
    "# parse data\n",
    "data_parser = Custom_data_set(dict_path, phoneme_dict_path)\n",
    "audio, phoneme_idx = data_parser.parse(audio_paths[0], transcript_paths[0])\n",
    "\n",
    "# load model\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(\"Device:\", device)\n",
    "target = 'vocals'\n",
    "\n",
    "# load model\n",
    "model_to_test = testx.load_model(target, model_path, device)\n",
    "model_to_test.return_alphas = True\n",
    "model_to_test.eval()\n",
    "\n",
    "# load model config\n",
    "with open(os.path.join(model_path, target + '.json'), 'r') as stream:\n",
    "    config = json.load(stream)\n",
    "    samplerate = config['args']['samplerate']\n",
    "    text_units = config['args']['text_units']\n",
    "    nfft = config['args']['nfft']\n",
    "    nhop = config['args']['nhop']\n",
    "\n",
    "with torch.no_grad():\n",
    "    vocals_estimate, alphas, scores = model_to_test((audio, phoneme_idx))\n",
    "\n",
    "optimal_path_scores = optimal_alignment_path(scores, mode='max_numpy', init=200)\n",
    "\n",
    "phoneme_onsets = compute_phoneme_onsets(optimal_path_scores, hop_length=nhop, sampling_rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_list = data_parser.get_phonemes(phoneme_idx[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', 'S', '>', 'W', '>', 'IY', '>', 'T', '>', 'CH', '>', 'AY', '>', 'L', '>', 'D', '>', 'IH', '>', 'N', '>', 'T', '>', 'AY', '>', 'M', '>', 'Y', '>', 'UW', '>', 'L', '>', 'S', '>', 'IY', '>', 'DH', '>', 'AH', '>', 'L', '>', 'AY', '>', 'N', '>', 'DH', '>', 'AH', '>', 'L', '>', 'AY', '>', 'N', '>', 'DH', '>', 'AE', '>', 'T', '>', 'S', '>', 'D', '>', 'R', '>', 'AO', '>', 'N', '>', 'B', '>', 'IH', '>', 'T', '>', 'W', '>', 'IY', '>', 'N', '>', 'G', '>', 'UH', '>', 'D', '>', 'AH', '>', 'N', '>', 'D', '>', 'B', '>', 'AE', '>', 'D', '>', 'S', '>', 'IY', '>', 'DH', '>', 'AH', '>', 'B', '>', 'L', '>', 'AY', '>', 'N', '>', 'D', '>', 'M', '>', 'AE', '>', 'N', '>', 'SH', '>', 'UW', '>', 'T', '>', 'IH', '>', 'NG', '>', 'AE', '>', 'T', '>', 'DH', '>', 'AH', '>', 'W', '>', 'ER', '>', 'L', '>', 'D', '>', 'B', '>', 'UH', '>', 'L', '>', 'AH', '>', 'T', '>', 'S', '>', 'F', '>', 'L', '>', 'AY', '>', 'IH', '>', 'NG', '>', 'OW', '>', 'T', '>', 'EY', '>', 'K', '>', 'IH', '>', 'NG', '>', 'T', '>', 'OW', '>', 'L', '$']\n"
     ]
    }
   ],
   "source": [
    "print(phoneme_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jali_sing",
   "language": "python",
   "name": "jali_sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
